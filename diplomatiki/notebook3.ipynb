{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import  Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "import keras\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "from imblearn.under_sampling import AllKNN\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as pip\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.calibration import calibration_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_df=pd.read_csv(\"C:\\\\BIG DATA\\\\thesis\\\\12045261\\\\Myocardial infarction complications Database.csv\")\n",
    "mic_df\n",
    "mic_df.loc[mic_df.LET_IS!=0,'LET_IS']=1 #To LET_IS einai multiclass. Ayti i grammi to kanei binary 0=zisane. 1=pethanane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Οι στηλες IBS_NASL, KFK_BLOOD,ΚΑΘΩΣ ΚΑΙ ΟΙ ,S_AD_KBRIG,D_AD_KBRIG αποτελουνται σχεδον αποκλειστικα απο missing values και γινονται drop. Drop kanoume episis kai to id\n",
    "mic_df.drop([\"ID\",\"IBS_NASL\",\"KFK_BLOOD\",\"S_AD_KBRIG\",\"D_AD_KBRIG\"],inplace=True,axis=1)\n",
    "#mic_df.drop([\"ID\",\"IBS_NASL\",\"KFK_BLOOD\"],inplace=True,axis=1)\n",
    "numeric_features=[\"AGE\",\"S_AD_ORIT\",\"D_AD_ORIT\",\"K_BLOOD\",\"NA_BLOOD\",\"ALT_BLOOD\",\"AST_BLOOD\",\"L_BLOOD\",\"ROE\",\"INF_ANAM\",\"STENOK_AN\",\"FK_STENOK\",\"GB\",\"DLIT_AG\",\"TIME_B_S\",\"R_AB_1_n\",\"R_AB_2_n\",\"R_AB_3_n\",\"NA_R_1_n\",\"NA_R_2_n\",\"NA_R_3_n\",\"NOT_NA_1_n\",\"NOT_NA_2_n\",\"NOT_NA_3_n\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable=\"LET_IS\"\n",
    "scenario=4\n",
    "to_drop_list3=[\"R_AB_3_n\",\"NA_R_3_n\",\"NOT_NA_3_n\"]\n",
    "to_drop_list2=[\"R_AB_2_n\",\"NA_R_2_n\",\"NOT_NA_2_n\"]\n",
    "to_drop_list1=[\"R_AB_1_n\",\"NA_R_1_n\",\"NOT_NA_1_n\"]\n",
    "to_drop_list2.extend(to_drop_list3)\n",
    "to_drop_list1.extend(to_drop_list2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>INF_ANAM</th>\n",
       "      <th>STENOK_AN</th>\n",
       "      <th>FK_STENOK</th>\n",
       "      <th>IBS_POST</th>\n",
       "      <th>GB</th>\n",
       "      <th>SIM_GIPERT</th>\n",
       "      <th>DLIT_AG</th>\n",
       "      <th>ZSN_A</th>\n",
       "      <th>...</th>\n",
       "      <th>NOT_NA_1_n</th>\n",
       "      <th>NOT_NA_2_n</th>\n",
       "      <th>NOT_NA_3_n</th>\n",
       "      <th>LID_S_n</th>\n",
       "      <th>B_BLOK_S_n</th>\n",
       "      <th>ANT_CA_S_n</th>\n",
       "      <th>GEPAR_S_n</th>\n",
       "      <th>ASP_S_n</th>\n",
       "      <th>TIKL_S_n</th>\n",
       "      <th>TRENT_S_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>79.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AGE  SEX  INF_ANAM  STENOK_AN  FK_STENOK  IBS_POST   GB  SIM_GIPERT  \\\n",
       "0     77.0    1       2.0        1.0        1.0       2.0  3.0         0.0   \n",
       "1     55.0    1       1.0        0.0        0.0       0.0  0.0         0.0   \n",
       "2     52.0    1       0.0        0.0        0.0       2.0  2.0         0.0   \n",
       "3     68.0    0       0.0        0.0        0.0       2.0  2.0         0.0   \n",
       "4     60.0    1       0.0        0.0        0.0       2.0  3.0         0.0   \n",
       "...    ...  ...       ...        ...        ...       ...  ...         ...   \n",
       "1695  77.0    0       0.0        4.0        2.0       1.0  2.0         0.0   \n",
       "1696  70.0    0       0.0        6.0        2.0       1.0  2.0         0.0   \n",
       "1697  55.0    1       3.0        6.0        2.0       2.0  0.0         0.0   \n",
       "1698  79.0    0       2.0        2.0        2.0       1.0  2.0         0.0   \n",
       "1699  63.0    1       2.0        NaN        NaN       NaN  2.0         0.0   \n",
       "\n",
       "      DLIT_AG  ZSN_A  ...  NOT_NA_1_n  NOT_NA_2_n  NOT_NA_3_n  LID_S_n  \\\n",
       "0         7.0    0.0  ...         0.0         0.0         0.0      1.0   \n",
       "1         0.0    0.0  ...         1.0         0.0         0.0      1.0   \n",
       "2         2.0    0.0  ...         3.0         2.0         2.0      1.0   \n",
       "3         3.0    1.0  ...         0.0         0.0         0.0      0.0   \n",
       "4         7.0    0.0  ...         0.0         0.0         0.0      0.0   \n",
       "...       ...    ...  ...         ...         ...         ...      ...   \n",
       "1695      7.0    0.0  ...         0.0         NaN         NaN      0.0   \n",
       "1696      7.0    0.0  ...         0.0         NaN         NaN      1.0   \n",
       "1697      0.0    0.0  ...         0.0         0.0         0.0      0.0   \n",
       "1698      7.0    NaN  ...         1.0         NaN         NaN      1.0   \n",
       "1699      NaN    4.0  ...         0.0         NaN         NaN      0.0   \n",
       "\n",
       "      B_BLOK_S_n  ANT_CA_S_n  GEPAR_S_n  ASP_S_n  TIKL_S_n  TRENT_S_n  \n",
       "0            0.0         0.0        1.0      1.0       0.0        0.0  \n",
       "1            0.0         1.0        1.0      1.0       0.0        1.0  \n",
       "2            1.0         0.0        1.0      1.0       0.0        0.0  \n",
       "3            0.0         1.0        1.0      1.0       0.0        0.0  \n",
       "4            0.0         1.0        0.0      1.0       0.0        1.0  \n",
       "...          ...         ...        ...      ...       ...        ...  \n",
       "1695         0.0         0.0        0.0      0.0       0.0        0.0  \n",
       "1696         0.0         0.0        0.0      0.0       0.0        0.0  \n",
       "1697         1.0         0.0        1.0      1.0       0.0        0.0  \n",
       "1698         0.0         1.0        1.0      1.0       0.0        0.0  \n",
       "1699         0.0         0.0        0.0      0.0       0.0        0.0  \n",
       "\n",
       "[1700 rows x 107 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=mic_df[target_variable]\n",
    "if scenario==5:\n",
    "    x = mic_df.drop([target_variable], axis=1)\n",
    "elif scenario==4:\n",
    "    x = mic_df.loc[: ,\"AGE\":\"TRENT_S_n\"]\n",
    "elif scenario==3:\n",
    "    df3 = mic_df.loc[: ,\"AGE\":\"TRENT_S_n\"]\n",
    "    x = df3.drop(to_drop_list3, axis=1)\n",
    "elif scenario==2:\n",
    "    df3 =  mic_df.loc[: ,\"AGE\":\"TRENT_S_n\"]\n",
    "    x = df3.drop(to_drop_list2, axis=1)\n",
    "elif scenario==1:\n",
    "    df3 = mic_df.loc[: ,\"AGE\":\"TRENT_S_n\"]\n",
    "    x = df3.drop(to_drop_list1, axis=1)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer=Pipeline(steps=[(\"imputer\",SimpleImputer(strategy=\"mean\")),(\"scaler\",StandardScaler())])\n",
    "l1=x.columns.values.tolist()#Παιρνω τα ονοματα ολων των στηλων σε λιστα\n",
    "\n",
    "categorical_features = [x for x in l1 if x not in numeric_features]#Απο την παραπανω λιστα αφαιρω τα ονοματα ΤΩΝ CONTINUOUS FEATURES\n",
    "\n",
    "categorical_transformer =Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\"))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)      \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"SVM\", SVC(kernel='linear'))])\n",
    "pipe2 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"Random Forest\", RandomForestClassifier(random_state=1234))])\n",
    "pipe3 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"Decision Tree\", DecisionTreeClassifier(random_state=1234))])\n",
    "pipe4 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"Logistic Regression\", LogisticRegression(solver='liblinear',random_state=1234,max_iter=10000))])\n",
    "pipe5 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"AdaBoost\", AdaBoostClassifier( random_state=1234))])\n",
    "pipe6 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"Gradient Boosting\", GradientBoostingClassifier( random_state=1234))])\n",
    "pipe7 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"XGboost\", XGBClassifier(use_label_encoder=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pca = PCA()\\npipe1 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"pca\",pca)])\\npipe1.fit(x)\\npca_data = pipe1.transform(x)\\n\\nper_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\\nlabels = [\\'PC\\' + str(x) for x in range(1, len(per_var)+1)]\\nplt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\\nplt.ylabel(\\'Percentage of Explained Variance\\')\\nplt.xlabel(\\'Principal Component\\')\\nplt.title(\\'Scree Plot\\')\\nplt.show()\\n\\n\\n#Ο παρακατω κωδικα μας δινει αθροιστικη διακυμανση ανα πληθος principal components\\nplt.figure( figsize = (10,8))\\nplt.plot(range(1,len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_.cumsum(), marker = \\'o\\', linestyle = \\'--\\', alpha=0.4)\\nplt.title(\"Explained Variance by Components\")\\nplt.xlabel(\"Number of Components\")\\nplt.ylabel(\"Cumulative Explained Variance\")'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA ME PIPELINE!\n",
    "'''pca = PCA()\n",
    "pipe1 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"pca\",pca)])\n",
    "pipe1.fit(x)\n",
    "pca_data = pipe1.transform(x)\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Ο παρακατω κωδικα μας δινει αθροιστικη διακυμανση ανα πληθος principal components\n",
    "plt.figure( figsize = (10,8))\n",
    "plt.plot(range(1,len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--', alpha=0.4)\n",
    "plt.title(\"Explained Variance by Components\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipe1 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"SVM\", SVC(kernel=\\'linear\\'))])\\npipe2 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"Random Forest\", RandomForestClassifier(random_state=1234))])\\npipe3 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"Decision Tree\", DecisionTreeClassifier(random_state=1234))])\\npipe4 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"Logistic Regression\", LogisticRegression(solver=\\'liblinear\\',random_state=1234,max_iter=10000))])\\npipe5 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"AdaBoost\", AdaBoostClassifier( random_state=1234))])\\npipe6 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"Gradient Boosting\", GradientBoostingClassifier( random_state=1234))])\\npipe7 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"XGboost\", XGBClassifier(use_label_encoder=False))])'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pipelines me PCA\n",
    "'''pipe1 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"SVM\", SVC(kernel='linear'))])\n",
    "pipe2 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"Random Forest\", RandomForestClassifier(random_state=1234))])\n",
    "pipe3 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"Decision Tree\", DecisionTreeClassifier(random_state=1234))])\n",
    "pipe4 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"Logistic Regression\", LogisticRegression(solver='liblinear',random_state=1234,max_iter=10000))])\n",
    "pipe5 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"AdaBoost\", AdaBoostClassifier( random_state=1234))])\n",
    "pipe6 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"Gradient Boosting\", GradientBoostingClassifier( random_state=1234))])\n",
    "pipe7 = Pipeline(steps=[(\"preprocessor\", preprocessor),(\"pca\",PCA(20)), (\"XGboost\", XGBClassifier(use_label_encoder=False))])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for col in numeric_features:\\n        mic_df[col].fillna(value=mic_df[col].mean(),inplace=True)\\n        \\n#Αντικατασταση missing values με most common value (=mode) για τις υπολοιπες στηλες\\nmic_df=mic_df.fillna(mic_df.mode().iloc[0])\\n\\n\\n#min-max scaler\\n#X_train_norm=X_train.copy()\\n#X_test_norm=X_test.copy()\\nfor c in numeric_features:\\n    norm=MinMaxScaler().fit(mic_df[[c]])    \\n    mic_df[c]=norm.transform(mic_df[[c]])\\n    mic_df[c]=norm.transform(mic_df[[c]])\\n\\n\\ncor=mic_df.corr()\\ncor_target = abs(cor[target_variable])\\nrelevant_features = cor_target[cor_target>0.3]\\nrelevant_features'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking for correlations... No important correlations found!\n",
    "'''for col in numeric_features:\n",
    "        mic_df[col].fillna(value=mic_df[col].mean(),inplace=True)\n",
    "        \n",
    "#Αντικατασταση missing values με most common value (=mode) για τις υπολοιπες στηλες\n",
    "mic_df=mic_df.fillna(mic_df.mode().iloc[0])\n",
    "\n",
    "\n",
    "#min-max scaler\n",
    "#X_train_norm=X_train.copy()\n",
    "#X_test_norm=X_test.copy()\n",
    "for c in numeric_features:\n",
    "    norm=MinMaxScaler().fit(mic_df[[c]])    \n",
    "    mic_df[c]=norm.transform(mic_df[[c]])\n",
    "    mic_df[c]=norm.transform(mic_df[[c]])\n",
    "\n",
    "\n",
    "cor=mic_df.corr()\n",
    "cor_target = abs(cor[target_variable])\n",
    "relevant_features = cor_target[cor_target>0.3]\n",
    "relevant_features'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "accuracy=0.875\n",
      "precision=0.677\n",
      "recall=0.424\n",
      "f1=0.517\n",
      "Random Forest\n",
      "accuracy=0.913\n",
      "precision=0.979\n",
      "recall=0.464\n",
      "f1=0.624\n",
      "Decision Tree\n",
      "accuracy=0.860\n",
      "precision=0.557\n",
      "recall=0.592\n",
      "f1=0.571\n",
      "Logistic Regression\n",
      "accuracy=0.873\n",
      "precision=0.657\n",
      "recall=0.440\n",
      "f1=0.521\n",
      "AdaBoost\n",
      "accuracy=0.913\n",
      "precision=0.808\n",
      "recall=0.607\n",
      "f1=0.689\n",
      "Gradient Boosting\n",
      "accuracy=0.917\n",
      "precision=0.863\n",
      "recall=0.576\n",
      "f1=0.685\n",
      "[11:29:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGboost\n",
      "accuracy=0.920\n",
      "precision=0.872\n",
      "recall=0.586\n",
      "f1=0.695\n"
     ]
    }
   ],
   "source": [
    "pipes=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6,pipe7]\n",
    "scoring = {'accuracy': 'accuracy','precision': 'precision','recall': 'recall','f1': 'f1'}\n",
    "names=['SVM','Random Forest','Decision Tree','Logistic Regression','AdaBoost','Gradient Boosting','XGboost']    \n",
    "    \n",
    "    # define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "for pipe,name in zip(pipes,names):\n",
    "    scores=cross_validate(pipe,x,y,cv=cv,scoring=scoring )\n",
    "    print(name)\n",
    "    print(\"accuracy=%.3f\" % scores[\"test_accuracy\"].mean())\n",
    "    print(\"precision=%.3f\" % scores[\"test_precision\"].mean())\n",
    "    print(\"recall=%.3f\" % scores[\"test_recall\"].mean())\n",
    "    print(\"f1=%.3f\" % scores[\"test_f1\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER TUNING WITH GRISEARCHCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define param grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_c_range=np.arange(0.01,0.2,0.01)\n",
    "param_grid1 = {\"SVM__C\":svm_c_range}\n",
    "\n",
    "class_weights=['balanced','balanced_subsample']\n",
    "max_depth_range=np.arange(2,10)\n",
    "param_grid2 = {\"Random Forest__max_depth\":max_depth_range,\"Random Forest__class_weight\":class_weights}\n",
    "\n",
    "max_leaf_nodes_range=np.arange(2,8)\n",
    "param_grid3 = {\"Decision Tree__max_depth\":max_depth_range,\"Decision Tree__max_leaf_nodes\":max_leaf_nodes_range}\n",
    "\n",
    "log_reg_pen=[\"l1\",\"l2\"]\n",
    "log_reg_c=np.arange(0.1,0.5,0.05)\n",
    "param_grid4 = {\"Logistic Regression__penalty\":log_reg_pen,\"Logistic Regression__C\":log_reg_c}\n",
    "\n",
    "estimators=np.arange(30,200,10)\n",
    "param_grid5 ={\"AdaBoost__n_estimators\":estimators}\n",
    "\n",
    "grad_boost_lrate=np.arange(0.05,0.15,0.01)\n",
    "param_grid6 ={\"Gradient Boosting__n_estimators\":estimators,\"Gradient Boosting__learning_rate\":grad_boost_lrate}\n",
    "\n",
    "scale_pos_weight=np.arange(1,7)\n",
    "param_grid7 ={\"XGboost__scale_pos_weight\":scale_pos_weight}#,\"XGboost__max_depth\":max_depth_range,\"XGboost__learning_rate\":grad_boost_lrate,\"XGboost__n_estimators\":estimators}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipelines(pipe1, pipe2, pipe3, pipe4, pipe5,pipe6,pipe7):\n",
    "#Me tin parakatw grammi orizeis 1 metriki tin opoia de mporeis na oriseis xerata opws tis upoloipes kathws den anikei stis dunates times tou scoring\n",
    "#scoring = {'acc': 'accuracy','precision': 'precision','recall': 'recall','f1': 'f1',\"matt\":make_scorer(matthews_corrcoef)}\n",
    "    scoring = {'accuracy': 'accuracy','precision': 'precision','recall': 'recall','f1': 'f1'}\n",
    "    \n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define grid\n",
    "    for pgrid, est in zip((param_grid1, param_grid2,\n",
    "                                param_grid3, param_grid4, param_grid5,param_grid6,param_grid7),\n",
    "                                (pipe1, pipe2, pipe3, pipe4, pipe5,pipe6,pipe7)):  \n",
    "        \n",
    "        # define grid search\n",
    "        grid = GridSearchCV(estimator=est, param_grid=pgrid, n_jobs=-1, cv=cv,scoring=scoring,refit=\"accuracy\")\n",
    "        # execute the grid search\n",
    "        grid_result = grid.fit(x, y)\n",
    "        # report the best configuration\n",
    "        #print(c[\"name\"])\n",
    "        print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_))\n",
    "        \n",
    "        # report all configurations\n",
    "        for scorer in zip(sorted(scoring)):\n",
    "            print(scorer)\n",
    "            means = grid_result.cv_results_['mean_test_%s' % scorer]\n",
    "            stds = grid_result.cv_results_['std_test_%s' % scorer]\n",
    "            params = grid_result.cv_results_['params']\n",
    "\n",
    "            for mean, stdev, param in zip(means, stds, params):\n",
    "                \n",
    "                print('%f (%f) with: %r' % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pipe1 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('selector', VarianceThreshold(.05)),(\"scaler\", StandardScaler()),(\"SVM\", SVC(kernel='linear'))])\n",
    "pipe2 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('selector', VarianceThreshold(.05)),(\"Random Forest\", RandomForestClassifier(random_state=1234))])\n",
    "pipe3 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('selector', VarianceThreshold(.05)),(\"Decision Tree\", DecisionTreeClassifier(random_state=1234))])\n",
    "pipe4 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('selector', VarianceThreshold(.05)),(\"Logistic Regression\", LogisticRegression(solver='liblinear',random_state=1234,max_iter=10000))])\n",
    "pipe5 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('selector', VarianceThreshold(.05)),(\"scaler\", StandardScaler()), (\"KNN\", KNeighborsClassifier())])\n",
    "pipe6 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('selector', VarianceThreshold(.05)),(\"AdaBoost\", AdaBoostClassifier( random_state=1234))])\n",
    "pipe7 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('selector', VarianceThreshold(.05)),(\"Gradient Boosting\", GradientBoostingClassifier( random_state=1234))])\n",
    "pipe8 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('selector', VarianceThreshold(.05)),(\"XGboost\", XGBClassifier(use_label_encoder=False))])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.879020 using {'SVM__C': 0.17}\n",
      "('accuracy',)\n",
      "0.867059 (0.016117) with: {'SVM__C': 0.01}\n",
      "0.871961 (0.020302) with: {'SVM__C': 0.02}\n",
      "0.873922 (0.020472) with: {'SVM__C': 0.03}\n",
      "0.873725 (0.022411) with: {'SVM__C': 0.04}\n",
      "0.875686 (0.022565) with: {'SVM__C': 0.05}\n",
      "0.876667 (0.024560) with: {'SVM__C': 0.060000000000000005}\n",
      "0.877451 (0.025824) with: {'SVM__C': 0.06999999999999999}\n",
      "0.877451 (0.025824) with: {'SVM__C': 0.08}\n",
      "0.876863 (0.025593) with: {'SVM__C': 0.09}\n",
      "0.877843 (0.025080) with: {'SVM__C': 0.09999999999999999}\n",
      "0.877647 (0.023938) with: {'SVM__C': 0.11}\n",
      "0.877647 (0.024225) with: {'SVM__C': 0.12}\n",
      "0.877647 (0.024790) with: {'SVM__C': 0.13}\n",
      "0.878235 (0.024638) with: {'SVM__C': 0.14}\n",
      "0.878431 (0.024647) with: {'SVM__C': 0.15000000000000002}\n",
      "0.878627 (0.023893) with: {'SVM__C': 0.16}\n",
      "0.879020 (0.023317) with: {'SVM__C': 0.17}\n",
      "0.877843 (0.023317) with: {'SVM__C': 0.18000000000000002}\n",
      "0.876275 (0.023108) with: {'SVM__C': 0.19}\n",
      "('f1',)\n",
      "0.320925 (0.119600) with: {'SVM__C': 0.01}\n",
      "0.382188 (0.137118) with: {'SVM__C': 0.02}\n",
      "0.419730 (0.128425) with: {'SVM__C': 0.03}\n",
      "0.429806 (0.131478) with: {'SVM__C': 0.04}\n",
      "0.448029 (0.129163) with: {'SVM__C': 0.05}\n",
      "0.465098 (0.126661) with: {'SVM__C': 0.060000000000000005}\n",
      "0.474322 (0.129723) with: {'SVM__C': 0.06999999999999999}\n",
      "0.478956 (0.127956) with: {'SVM__C': 0.08}\n",
      "0.477071 (0.129393) with: {'SVM__C': 0.09}\n",
      "0.480400 (0.129202) with: {'SVM__C': 0.09999999999999999}\n",
      "0.479095 (0.126269) with: {'SVM__C': 0.11}\n",
      "0.480979 (0.122077) with: {'SVM__C': 0.12}\n",
      "0.482729 (0.123203) with: {'SVM__C': 0.13}\n",
      "0.488493 (0.120559) with: {'SVM__C': 0.14}\n",
      "0.490939 (0.121987) with: {'SVM__C': 0.15000000000000002}\n",
      "0.491105 (0.119575) with: {'SVM__C': 0.16}\n",
      "0.496532 (0.115210) with: {'SVM__C': 0.17}\n",
      "0.493454 (0.114052) with: {'SVM__C': 0.18000000000000002}\n",
      "0.487981 (0.112243) with: {'SVM__C': 0.19}\n",
      "('precision',)\n",
      "0.821891 (0.213263) with: {'SVM__C': 0.01}\n",
      "0.791530 (0.205114) with: {'SVM__C': 0.02}\n",
      "0.766681 (0.161015) with: {'SVM__C': 0.03}\n",
      "0.752380 (0.173682) with: {'SVM__C': 0.04}\n",
      "0.755942 (0.159411) with: {'SVM__C': 0.05}\n",
      "0.749814 (0.161660) with: {'SVM__C': 0.060000000000000005}\n",
      "0.744093 (0.161289) with: {'SVM__C': 0.06999999999999999}\n",
      "0.738307 (0.158696) with: {'SVM__C': 0.08}\n",
      "0.730529 (0.163732) with: {'SVM__C': 0.09}\n",
      "0.736636 (0.159213) with: {'SVM__C': 0.09999999999999999}\n",
      "0.736430 (0.155825) with: {'SVM__C': 0.11}\n",
      "0.738745 (0.156343) with: {'SVM__C': 0.12}\n",
      "0.736444 (0.159189) with: {'SVM__C': 0.13}\n",
      "0.737072 (0.156960) with: {'SVM__C': 0.14}\n",
      "0.733972 (0.154858) with: {'SVM__C': 0.15000000000000002}\n",
      "0.736305 (0.148664) with: {'SVM__C': 0.16}\n",
      "0.733626 (0.142123) with: {'SVM__C': 0.17}\n",
      "0.726201 (0.145049) with: {'SVM__C': 0.18000000000000002}\n",
      "0.716357 (0.142567) with: {'SVM__C': 0.19}\n",
      "('recall',)\n",
      "0.204277 (0.088977) with: {'SVM__C': 0.01}\n",
      "0.259524 (0.111831) with: {'SVM__C': 0.02}\n",
      "0.297663 (0.114958) with: {'SVM__C': 0.03}\n",
      "0.310009 (0.116916) with: {'SVM__C': 0.04}\n",
      "0.328483 (0.118770) with: {'SVM__C': 0.05}\n",
      "0.346825 (0.118533) with: {'SVM__C': 0.060000000000000005}\n",
      "0.357760 (0.123032) with: {'SVM__C': 0.06999999999999999}\n",
      "0.363933 (0.121832) with: {'SVM__C': 0.08}\n",
      "0.362743 (0.120692) with: {'SVM__C': 0.09}\n",
      "0.365212 (0.121201) with: {'SVM__C': 0.09999999999999999}\n",
      "0.364021 (0.118919) with: {'SVM__C': 0.11}\n",
      "0.365300 (0.115054) with: {'SVM__C': 0.12}\n",
      "0.367769 (0.116714) with: {'SVM__C': 0.13}\n",
      "0.373898 (0.114705) with: {'SVM__C': 0.14}\n",
      "0.377601 (0.117098) with: {'SVM__C': 0.15000000000000002}\n",
      "0.377601 (0.116707) with: {'SVM__C': 0.16}\n",
      "0.383686 (0.112844) with: {'SVM__C': 0.17}\n",
      "0.382451 (0.112374) with: {'SVM__C': 0.18000000000000002}\n",
      "0.378836 (0.111673) with: {'SVM__C': 0.19}\n",
      "Best: 0.914314 using {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 8}\n",
      "('accuracy',)\n",
      "0.876078 (0.020146) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 2}\n",
      "0.887843 (0.020088) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 3}\n",
      "0.899412 (0.018374) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 4}\n",
      "0.905882 (0.017712) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 5}\n",
      "0.912353 (0.017929) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 6}\n",
      "0.913137 (0.017822) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 7}\n",
      "0.912157 (0.016069) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 8}\n",
      "0.912745 (0.016504) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 9}\n",
      "0.875882 (0.020055) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 2}\n",
      "0.888039 (0.021290) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 3}\n",
      "0.900588 (0.019707) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 4}\n",
      "0.906078 (0.017613) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 5}\n",
      "0.912549 (0.018709) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 6}\n",
      "0.912941 (0.016666) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 7}\n",
      "0.914314 (0.017363) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 8}\n",
      "0.913137 (0.014857) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 9}\n",
      "('f1',)\n",
      "0.603466 (0.065353) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 2}\n",
      "0.625302 (0.070628) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 3}\n",
      "0.647438 (0.071775) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 4}\n",
      "0.656254 (0.076994) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 5}\n",
      "0.663576 (0.080583) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 6}\n",
      "0.649862 (0.089752) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 7}\n",
      "0.633168 (0.084222) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 8}\n",
      "0.630778 (0.085142) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 9}\n",
      "0.603708 (0.064039) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 2}\n",
      "0.626877 (0.073137) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 3}\n",
      "0.651961 (0.074237) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 4}\n",
      "0.660341 (0.073357) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 5}\n",
      "0.662380 (0.085658) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 6}\n",
      "0.648492 (0.082702) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 7}\n",
      "0.645188 (0.089106) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 8}\n",
      "0.632661 (0.079168) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 9}\n",
      "('precision',)\n",
      "0.619293 (0.069624) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 2}\n",
      "0.672992 (0.084873) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 3}\n",
      "0.736807 (0.085792) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 4}\n",
      "0.785899 (0.085638) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 5}\n",
      "0.852931 (0.090409) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 6}\n",
      "0.894878 (0.066393) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 7}\n",
      "0.937234 (0.074192) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 8}\n",
      "0.956156 (0.066813) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 9}\n",
      "0.619314 (0.072922) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 2}\n",
      "0.673835 (0.089357) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 3}\n",
      "0.740245 (0.087895) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 4}\n",
      "0.781688 (0.085951) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 5}\n",
      "0.855121 (0.084828) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 6}\n",
      "0.901922 (0.074696) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 7}\n",
      "0.933593 (0.064357) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 8}\n",
      "0.958225 (0.053868) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 9}\n",
      "('recall',)\n",
      "0.594312 (0.083606) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 2}\n",
      "0.590608 (0.087606) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 3}\n",
      "0.584436 (0.087510) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 4}\n",
      "0.572178 (0.096927) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 5}\n",
      "0.550088 (0.092583) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 6}\n",
      "0.517063 (0.101319) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 7}\n",
      "0.485053 (0.092044) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 8}\n",
      "0.476411 (0.090324) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 9}\n",
      "0.595503 (0.082502) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 2}\n",
      "0.593078 (0.088665) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 3}\n",
      "0.588139 (0.085934) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 4}\n",
      "0.579586 (0.090943) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 5}\n",
      "0.547751 (0.099507) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 6}\n",
      "0.513404 (0.094153) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 7}\n",
      "0.499780 (0.099121) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 8}\n",
      "0.477601 (0.085568) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 9}\n",
      "Best: 0.909804 using {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 3}\n",
      "('accuracy',)\n",
      "0.878431 (0.017139) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.878431 (0.017139) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.908824 (0.016127) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.907843 (0.016661) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.907255 (0.017095) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.878431 (0.017139) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.908824 (0.016127) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905882 (0.016217) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.904314 (0.016702) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.878431 (0.017139) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.908824 (0.016127) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905882 (0.016217) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.904118 (0.016855) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.878431 (0.017139) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.908824 (0.016127) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905882 (0.016217) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.904118 (0.016855) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.878431 (0.017139) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.908824 (0.016127) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905882 (0.016217) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.904118 (0.016855) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.878431 (0.017139) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.908824 (0.016127) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905882 (0.016217) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.904118 (0.016855) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.878431 (0.017139) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.909804 (0.015881) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.908824 (0.016127) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905882 (0.016217) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.904118 (0.016855) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 7}\n",
      "('f1',)\n",
      "0.579630 (0.067348) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.579630 (0.067348) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.611975 (0.087638) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.610068 (0.089013) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.610093 (0.088873) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.579630 (0.067348) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.611975 (0.087638) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.605067 (0.086649) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.601468 (0.086240) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.579630 (0.067348) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.611975 (0.087638) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.605067 (0.086649) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.601810 (0.085961) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.579630 (0.067348) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.611975 (0.087638) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.605067 (0.086649) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.601810 (0.085961) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.579630 (0.067348) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.611975 (0.087638) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.605067 (0.086649) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.601810 (0.085961) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.579630 (0.067348) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.611975 (0.087638) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.605067 (0.086649) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.601810 (0.085961) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.579630 (0.067348) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.609146 (0.089390) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.611975 (0.087638) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.605067 (0.086649) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.601810 (0.085961) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 7}\n",
      "('precision',)\n",
      "0.646399 (0.065998) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.646399 (0.065998) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.937791 (0.064279) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.924247 (0.072652) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.914489 (0.080769) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.646399 (0.065998) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.937791 (0.064279) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905479 (0.079391) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.889415 (0.087293) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.646399 (0.065998) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.937791 (0.064279) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905479 (0.079391) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.885302 (0.089155) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.646399 (0.065998) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.937791 (0.064279) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905479 (0.079391) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.885302 (0.089155) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.646399 (0.065998) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.937791 (0.064279) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905479 (0.079391) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.885302 (0.089155) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.646399 (0.065998) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.937791 (0.064279) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905479 (0.079391) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.885302 (0.089155) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.646399 (0.065998) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966057 (0.047412) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.937791 (0.064279) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905479 (0.079391) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.885302 (0.089155) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 7}\n",
      "('recall',)\n",
      "0.530467 (0.083984) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.530467 (0.083984) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.460450 (0.092038) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.461684 (0.093542) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.464109 (0.092876) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.530467 (0.083984) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.460450 (0.092038) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.461684 (0.093542) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.461640 (0.092380) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.530467 (0.083984) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.460450 (0.092038) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.461684 (0.093542) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.462875 (0.091394) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.530467 (0.083984) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.460450 (0.092038) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.461684 (0.093542) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.462875 (0.091394) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.530467 (0.083984) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.460450 (0.092038) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.461684 (0.093542) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.462875 (0.091394) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.530467 (0.083984) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.460450 (0.092038) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.461684 (0.093542) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.462875 (0.091394) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.530467 (0.083984) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.450617 (0.092277) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.460450 (0.092038) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.461684 (0.093542) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.462875 (0.091394) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 7}\n",
      "Best: 0.881176 using {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l1'}\n",
      "('accuracy',)\n",
      "0.872353 (0.024213) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l1'}\n",
      "0.877647 (0.026148) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l2'}\n",
      "0.876078 (0.025457) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l1'}\n",
      "0.877059 (0.025340) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l2'}\n",
      "0.879216 (0.025674) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.876863 (0.025000) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.879020 (0.026293) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l1'}\n",
      "0.877647 (0.024415) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l2'}\n",
      "0.880392 (0.026277) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.877255 (0.024430) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.881176 (0.026368) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l1'}\n",
      "0.876471 (0.024490) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l2'}\n",
      "0.880000 (0.027067) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l1'}\n",
      "0.876275 (0.024134) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l2'}\n",
      "0.880000 (0.026025) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l1'}\n",
      "0.876275 (0.024419) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l2'}\n",
      "('f1',)\n",
      "0.440915 (0.128791) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l1'}\n",
      "0.502240 (0.127679) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l2'}\n",
      "0.476255 (0.131851) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l1'}\n",
      "0.508549 (0.122738) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l2'}\n",
      "0.499275 (0.131972) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.513540 (0.113672) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.502100 (0.132127) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l1'}\n",
      "0.520012 (0.108878) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l2'}\n",
      "0.510896 (0.132178) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.521802 (0.106993) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.516399 (0.131549) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l1'}\n",
      "0.522335 (0.106442) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l2'}\n",
      "0.515278 (0.133249) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l1'}\n",
      "0.522230 (0.106758) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l2'}\n",
      "0.521617 (0.124880) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l1'}\n",
      "0.523922 (0.106031) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l2'}\n",
      "('precision',)\n",
      "0.714352 (0.156887) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l1'}\n",
      "0.703356 (0.144179) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l2'}\n",
      "0.715068 (0.147244) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l1'}\n",
      "0.692008 (0.132696) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l2'}\n",
      "0.719350 (0.138213) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.692283 (0.134267) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.715843 (0.140715) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l1'}\n",
      "0.692360 (0.122459) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l2'}\n",
      "0.717720 (0.135497) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.688261 (0.123491) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.719293 (0.134700) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l1'}\n",
      "0.682094 (0.124200) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l2'}\n",
      "0.710397 (0.140352) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l1'}\n",
      "0.680004 (0.121786) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l2'}\n",
      "0.705975 (0.131624) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l1'}\n",
      "0.679355 (0.122768) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l2'}\n",
      "('recall',)\n",
      "0.324691 (0.113699) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l1'}\n",
      "0.398457 (0.125090) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l2'}\n",
      "0.365300 (0.125747) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l1'}\n",
      "0.410758 (0.125502) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l2'}\n",
      "0.391093 (0.130867) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.416887 (0.116492) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.395944 (0.131837) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l1'}\n",
      "0.424250 (0.112574) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l2'}\n",
      "0.405776 (0.133486) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.427954 (0.110518) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.411905 (0.133696) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l1'}\n",
      "0.431658 (0.111214) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l2'}\n",
      "0.414330 (0.136046) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l1'}\n",
      "0.432892 (0.112777) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l2'}\n",
      "0.422972 (0.128928) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l1'}\n",
      "0.435317 (0.111480) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l2'}\n",
      "Best: 0.917059 using {'AdaBoost__n_estimators': 40}\n",
      "('accuracy',)\n",
      "0.912941 (0.019236) with: {'AdaBoost__n_estimators': 30}\n",
      "0.917059 (0.015440) with: {'AdaBoost__n_estimators': 40}\n",
      "0.913333 (0.014403) with: {'AdaBoost__n_estimators': 50}\n",
      "0.913529 (0.016156) with: {'AdaBoost__n_estimators': 60}\n",
      "0.913529 (0.017916) with: {'AdaBoost__n_estimators': 70}\n",
      "0.914314 (0.016406) with: {'AdaBoost__n_estimators': 80}\n",
      "0.912549 (0.017366) with: {'AdaBoost__n_estimators': 90}\n",
      "0.910000 (0.019281) with: {'AdaBoost__n_estimators': 100}\n",
      "0.910784 (0.019514) with: {'AdaBoost__n_estimators': 110}\n",
      "0.911961 (0.018878) with: {'AdaBoost__n_estimators': 120}\n",
      "0.911373 (0.016633) with: {'AdaBoost__n_estimators': 130}\n",
      "0.909804 (0.015439) with: {'AdaBoost__n_estimators': 140}\n",
      "0.910588 (0.016386) with: {'AdaBoost__n_estimators': 150}\n",
      "0.910392 (0.018829) with: {'AdaBoost__n_estimators': 160}\n",
      "0.910588 (0.017210) with: {'AdaBoost__n_estimators': 170}\n",
      "0.908039 (0.017283) with: {'AdaBoost__n_estimators': 180}\n",
      "0.907647 (0.016369) with: {'AdaBoost__n_estimators': 190}\n",
      "('f1',)\n",
      "0.688262 (0.072477) with: {'AdaBoost__n_estimators': 30}\n",
      "0.702784 (0.056389) with: {'AdaBoost__n_estimators': 40}\n",
      "0.689309 (0.055487) with: {'AdaBoost__n_estimators': 50}\n",
      "0.689625 (0.069164) with: {'AdaBoost__n_estimators': 60}\n",
      "0.689840 (0.073779) with: {'AdaBoost__n_estimators': 70}\n",
      "0.690836 (0.070620) with: {'AdaBoost__n_estimators': 80}\n",
      "0.686361 (0.076153) with: {'AdaBoost__n_estimators': 90}\n",
      "0.678694 (0.075037) with: {'AdaBoost__n_estimators': 100}\n",
      "0.680224 (0.078116) with: {'AdaBoost__n_estimators': 110}\n",
      "0.685603 (0.071823) with: {'AdaBoost__n_estimators': 120}\n",
      "0.683193 (0.069253) with: {'AdaBoost__n_estimators': 130}\n",
      "0.678112 (0.065997) with: {'AdaBoost__n_estimators': 140}\n",
      "0.681200 (0.065388) with: {'AdaBoost__n_estimators': 150}\n",
      "0.682240 (0.071824) with: {'AdaBoost__n_estimators': 160}\n",
      "0.685519 (0.065193) with: {'AdaBoost__n_estimators': 170}\n",
      "0.677182 (0.068811) with: {'AdaBoost__n_estimators': 180}\n",
      "0.674042 (0.067199) with: {'AdaBoost__n_estimators': 190}\n",
      "('precision',)\n",
      "0.807116 (0.093764) with: {'AdaBoost__n_estimators': 30}\n",
      "0.825390 (0.084197) with: {'AdaBoost__n_estimators': 40}\n",
      "0.808109 (0.072225) with: {'AdaBoost__n_estimators': 50}\n",
      "0.803105 (0.063783) with: {'AdaBoost__n_estimators': 60}\n",
      "0.802567 (0.071925) with: {'AdaBoost__n_estimators': 70}\n",
      "0.812345 (0.072820) with: {'AdaBoost__n_estimators': 80}\n",
      "0.799015 (0.070630) with: {'AdaBoost__n_estimators': 90}\n",
      "0.787290 (0.081407) with: {'AdaBoost__n_estimators': 100}\n",
      "0.793857 (0.087630) with: {'AdaBoost__n_estimators': 110}\n",
      "0.797653 (0.085662) with: {'AdaBoost__n_estimators': 120}\n",
      "0.793799 (0.074432) with: {'AdaBoost__n_estimators': 130}\n",
      "0.787433 (0.074078) with: {'AdaBoost__n_estimators': 140}\n",
      "0.790700 (0.080141) with: {'AdaBoost__n_estimators': 150}\n",
      "0.786751 (0.088176) with: {'AdaBoost__n_estimators': 160}\n",
      "0.784664 (0.086420) with: {'AdaBoost__n_estimators': 170}\n",
      "0.768305 (0.074802) with: {'AdaBoost__n_estimators': 180}\n",
      "0.769955 (0.071867) with: {'AdaBoost__n_estimators': 190}\n",
      "('recall',)\n",
      "0.606481 (0.082271) with: {'AdaBoost__n_estimators': 30}\n",
      "0.617549 (0.069202) with: {'AdaBoost__n_estimators': 40}\n",
      "0.606658 (0.071756) with: {'AdaBoost__n_estimators': 50}\n",
      "0.610450 (0.089208) with: {'AdaBoost__n_estimators': 60}\n",
      "0.610406 (0.090223) with: {'AdaBoost__n_estimators': 70}\n",
      "0.609171 (0.093163) with: {'AdaBoost__n_estimators': 80}\n",
      "0.610538 (0.099816) with: {'AdaBoost__n_estimators': 90}\n",
      "0.601808 (0.088352) with: {'AdaBoost__n_estimators': 100}\n",
      "0.603042 (0.098301) with: {'AdaBoost__n_estimators': 110}\n",
      "0.606702 (0.083771) with: {'AdaBoost__n_estimators': 120}\n",
      "0.606746 (0.089758) with: {'AdaBoost__n_estimators': 130}\n",
      "0.602998 (0.086717) with: {'AdaBoost__n_estimators': 140}\n",
      "0.604145 (0.079557) with: {'AdaBoost__n_estimators': 150}\n",
      "0.607848 (0.083518) with: {'AdaBoost__n_estimators': 160}\n",
      "0.615300 (0.079383) with: {'AdaBoost__n_estimators': 170}\n",
      "0.610273 (0.082044) with: {'AdaBoost__n_estimators': 180}\n",
      "0.605423 (0.086462) with: {'AdaBoost__n_estimators': 190}\n",
      "Best: 0.920000 using {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "('accuracy',)\n",
      "0.910196 (0.016140) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 30}\n",
      "0.909804 (0.015954) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 40}\n",
      "0.910784 (0.017189) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 50}\n",
      "0.912549 (0.017299) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 60}\n",
      "0.913529 (0.016923) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 70}\n",
      "0.914902 (0.017166) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 80}\n",
      "0.913922 (0.017938) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 90}\n",
      "0.914706 (0.017826) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 100}\n",
      "0.915882 (0.018918) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 110}\n",
      "0.916863 (0.018272) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 120}\n",
      "0.916667 (0.018420) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 130}\n",
      "0.918235 (0.018808) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 140}\n",
      "0.918235 (0.017670) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 150}\n",
      "0.917647 (0.017317) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 160}\n",
      "0.918431 (0.016963) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 170}\n",
      "0.917647 (0.017183) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 180}\n",
      "0.917843 (0.016878) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 190}\n",
      "0.909608 (0.015964) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 30}\n",
      "0.911569 (0.016322) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 40}\n",
      "0.913333 (0.016702) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 50}\n",
      "0.914510 (0.017432) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 60}\n",
      "0.914902 (0.017889) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 70}\n",
      "0.916471 (0.018314) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 80}\n",
      "0.916471 (0.017673) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 90}\n",
      "0.915686 (0.018120) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 100}\n",
      "0.916863 (0.019015) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 110}\n",
      "0.917059 (0.018374) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 120}\n",
      "0.917451 (0.017548) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 130}\n",
      "0.916863 (0.017233) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 140}\n",
      "0.916667 (0.018607) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 150}\n",
      "0.916078 (0.017445) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 160}\n",
      "0.916667 (0.017718) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 170}\n",
      "0.916275 (0.017495) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 180}\n",
      "0.916078 (0.018659) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 190}\n",
      "0.910784 (0.015936) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 30}\n",
      "0.913137 (0.016685) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 40}\n",
      "0.914706 (0.016551) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 50}\n",
      "0.915098 (0.018206) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 60}\n",
      "0.916667 (0.018104) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 70}\n",
      "0.916863 (0.018461) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 80}\n",
      "0.918039 (0.017511) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 90}\n",
      "0.917451 (0.017349) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 100}\n",
      "0.917255 (0.016840) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 110}\n",
      "0.917647 (0.016776) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 120}\n",
      "0.917255 (0.016424) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 130}\n",
      "0.916667 (0.016850) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 140}\n",
      "0.916471 (0.017076) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 150}\n",
      "0.916863 (0.017233) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 160}\n",
      "0.916863 (0.016826) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 170}\n",
      "0.917059 (0.017207) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 180}\n",
      "0.917255 (0.018285) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 190}\n",
      "0.910980 (0.016895) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 30}\n",
      "0.913137 (0.016194) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 40}\n",
      "0.914706 (0.016551) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 50}\n",
      "0.914510 (0.018018) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 60}\n",
      "0.917451 (0.017679) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 70}\n",
      "0.917059 (0.019233) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 80}\n",
      "0.917451 (0.016740) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 90}\n",
      "0.917059 (0.017473) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 100}\n",
      "0.917451 (0.017938) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 110}\n",
      "0.917451 (0.018693) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 120}\n",
      "0.916863 (0.018335) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 130}\n",
      "0.917255 (0.018659) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 140}\n",
      "0.917255 (0.017773) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 150}\n",
      "0.917059 (0.017670) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 160}\n",
      "0.916863 (0.018709) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 170}\n",
      "0.917059 (0.018437) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 180}\n",
      "0.916667 (0.019096) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 190}\n",
      "0.911961 (0.016809) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.912745 (0.016364) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.913725 (0.016452) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.915098 (0.017692) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.916078 (0.017577) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.917451 (0.017809) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.916471 (0.017277) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.917451 (0.017548) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.917451 (0.018507) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.917059 (0.019112) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.917843 (0.019181) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.917255 (0.018966) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.918431 (0.019135) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.918431 (0.020531) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.919216 (0.019087) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.920000 (0.019127) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.918235 (0.020283) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.912353 (0.016241) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 30}\n",
      "0.914706 (0.016896) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 40}\n",
      "0.916275 (0.018079) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 50}\n",
      "0.917843 (0.017809) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 60}\n",
      "0.916667 (0.018607) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 70}\n",
      "0.916078 (0.017838) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 80}\n",
      "0.916667 (0.018545) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 90}\n",
      "0.917059 (0.017670) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 100}\n",
      "0.916471 (0.017277) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 110}\n",
      "0.917451 (0.016740) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 120}\n",
      "0.916863 (0.016963) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 130}\n",
      "0.916667 (0.018104) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 140}\n",
      "0.917059 (0.018248) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 150}\n",
      "0.917059 (0.017735) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 160}\n",
      "0.917451 (0.018319) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 170}\n",
      "0.917451 (0.018755) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 180}\n",
      "0.916863 (0.018893) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 190}\n",
      "0.912353 (0.016383) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.913333 (0.017112) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.916078 (0.019087) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.915882 (0.019100) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.915686 (0.018622) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.917647 (0.016707) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.918039 (0.016908) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.917451 (0.018319) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.918824 (0.017476) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.918039 (0.018348) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.916471 (0.017076) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.916471 (0.017210) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.916863 (0.017233) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.916078 (0.017902) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.915882 (0.017981) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.916275 (0.017561) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.917059 (0.018808) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.913529 (0.014659) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.913725 (0.016730) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.915294 (0.018946) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.916275 (0.018582) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.916275 (0.018706) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.915686 (0.018622) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.917255 (0.019148) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.916863 (0.018082) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.916667 (0.017718) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.915882 (0.019400) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.916078 (0.018411) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.916667 (0.016987) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.916667 (0.017847) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.916667 (0.016850) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.917647 (0.017712) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.915686 (0.016730) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.916863 (0.018018) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.913922 (0.017081) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 30}\n",
      "0.916275 (0.017757) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 40}\n",
      "0.916667 (0.018730) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 50}\n",
      "0.916471 (0.018626) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 60}\n",
      "0.918431 (0.017031) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 70}\n",
      "0.917843 (0.015673) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 80}\n",
      "0.918039 (0.016424) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 90}\n",
      "0.918824 (0.016457) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 100}\n",
      "0.918039 (0.016771) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 110}\n",
      "0.917255 (0.018222) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 120}\n",
      "0.917647 (0.017384) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 130}\n",
      "0.918627 (0.018104) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 140}\n",
      "0.917059 (0.018685) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 150}\n",
      "0.917059 (0.017735) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 160}\n",
      "0.915882 (0.018611) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 170}\n",
      "0.916863 (0.018398) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 180}\n",
      "0.917059 (0.019293) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 190}\n",
      "0.913529 (0.016084) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 30}\n",
      "0.916471 (0.018188) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 40}\n",
      "0.915098 (0.016754) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 50}\n",
      "0.917255 (0.017511) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 60}\n",
      "0.917843 (0.017014) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 70}\n",
      "0.916863 (0.017366) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 80}\n",
      "0.917059 (0.017005) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 90}\n",
      "0.917647 (0.016568) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 100}\n",
      "0.919020 (0.017363) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 110}\n",
      "0.918431 (0.017498) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 120}\n",
      "0.917059 (0.018057) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 130}\n",
      "0.917059 (0.019412) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 140}\n",
      "0.917059 (0.018991) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 150}\n",
      "0.915490 (0.019656) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 160}\n",
      "0.917451 (0.020177) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 170}\n",
      "0.917451 (0.020291) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 180}\n",
      "0.917059 (0.018991) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 190}\n",
      "('f1',)\n",
      "0.608448 (0.093716) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 30}\n",
      "0.609991 (0.093053) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 40}\n",
      "0.619471 (0.095474) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 50}\n",
      "0.633058 (0.094956) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 60}\n",
      "0.640268 (0.093758) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 70}\n",
      "0.650551 (0.092514) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 80}\n",
      "0.649878 (0.093659) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 90}\n",
      "0.656109 (0.093420) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 100}\n",
      "0.663703 (0.097512) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 110}\n",
      "0.669425 (0.092758) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 120}\n",
      "0.671382 (0.090405) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 130}\n",
      "0.680547 (0.089181) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 140}\n",
      "0.683209 (0.083758) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 150}\n",
      "0.681795 (0.083660) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 160}\n",
      "0.686155 (0.080724) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 170}\n",
      "0.684493 (0.081259) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 180}\n",
      "0.686049 (0.080086) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 190}\n",
      "0.608236 (0.092759) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 30}\n",
      "0.621682 (0.092402) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 40}\n",
      "0.636354 (0.093081) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 50}\n",
      "0.644912 (0.095602) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 60}\n",
      "0.651847 (0.094352) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 70}\n",
      "0.661236 (0.095651) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 80}\n",
      "0.664663 (0.091806) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 90}\n",
      "0.663162 (0.092828) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 100}\n",
      "0.671563 (0.094163) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 110}\n",
      "0.676870 (0.088939) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 120}\n",
      "0.679724 (0.084375) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 130}\n",
      "0.680083 (0.082574) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 140}\n",
      "0.679510 (0.088229) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 150}\n",
      "0.678634 (0.083289) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 160}\n",
      "0.681337 (0.083230) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 170}\n",
      "0.681412 (0.081384) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 180}\n",
      "0.681066 (0.086150) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 190}\n",
      "0.616111 (0.090561) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 30}\n",
      "0.632526 (0.095246) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 40}\n",
      "0.645248 (0.093675) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 50}\n",
      "0.651329 (0.095249) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 60}\n",
      "0.662992 (0.094151) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 70}\n",
      "0.668332 (0.092900) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 80}\n",
      "0.676411 (0.087702) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 90}\n",
      "0.676759 (0.085363) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 100}\n",
      "0.681271 (0.080584) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 110}\n",
      "0.683997 (0.079314) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 120}\n",
      "0.683708 (0.076974) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 130}\n",
      "0.683280 (0.078748) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 140}\n",
      "0.683101 (0.079725) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 150}\n",
      "0.685367 (0.078803) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 160}\n",
      "0.686150 (0.077385) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 170}\n",
      "0.688519 (0.077960) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 180}\n",
      "0.690555 (0.079680) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 190}\n",
      "0.619680 (0.093967) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 30}\n",
      "0.638376 (0.092463) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 40}\n",
      "0.649298 (0.091900) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 50}\n",
      "0.655313 (0.093870) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 60}\n",
      "0.670753 (0.093654) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 70}\n",
      "0.671187 (0.100865) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 80}\n",
      "0.677760 (0.084278) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 90}\n",
      "0.679095 (0.086564) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 100}\n",
      "0.683491 (0.087275) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 110}\n",
      "0.685507 (0.087114) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 120}\n",
      "0.684307 (0.087136) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 130}\n",
      "0.688726 (0.084792) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 140}\n",
      "0.690440 (0.079559) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 150}\n",
      "0.690643 (0.077798) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 160}\n",
      "0.689973 (0.084214) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 170}\n",
      "0.692076 (0.084099) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 180}\n",
      "0.690254 (0.085865) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 190}\n",
      "0.628447 (0.094093) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.638754 (0.091619) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.649102 (0.088144) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.660696 (0.090604) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.669407 (0.089740) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.678851 (0.084753) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.676919 (0.082158) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.682743 (0.084561) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.684796 (0.086305) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.685320 (0.086950) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.689680 (0.086316) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.687946 (0.084856) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.693103 (0.085197) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.695579 (0.088847) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.698957 (0.083689) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.702907 (0.084355) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.696124 (0.089293) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.633546 (0.090362) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 30}\n",
      "0.650429 (0.090939) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 40}\n",
      "0.662893 (0.096064) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 50}\n",
      "0.673984 (0.089941) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 60}\n",
      "0.675834 (0.088383) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 70}\n",
      "0.677911 (0.082721) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 80}\n",
      "0.682853 (0.084154) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 90}\n",
      "0.684803 (0.082145) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 100}\n",
      "0.685375 (0.078060) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 110}\n",
      "0.688830 (0.076353) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 120}\n",
      "0.687664 (0.077301) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 130}\n",
      "0.688806 (0.078950) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 140}\n",
      "0.691940 (0.079585) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 150}\n",
      "0.693848 (0.077245) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 160}\n",
      "0.696074 (0.078960) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 170}\n",
      "0.695259 (0.082013) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 180}\n",
      "0.693402 (0.081978) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 190}\n",
      "0.636631 (0.091452) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.649057 (0.089729) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.664995 (0.096377) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.671065 (0.096565) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.674098 (0.091846) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.684399 (0.076291) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.688626 (0.077609) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.688119 (0.081086) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.694852 (0.078302) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.693309 (0.081016) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.689284 (0.076520) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.691488 (0.076428) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.693694 (0.075086) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.690209 (0.081155) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.690690 (0.080462) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.692046 (0.076538) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.694915 (0.079941) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.644410 (0.083012) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.651190 (0.091822) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.665556 (0.095046) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.674615 (0.092671) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.678480 (0.093070) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.678590 (0.089391) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.686504 (0.091772) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.685758 (0.090659) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.686005 (0.088130) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.685147 (0.092391) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.687718 (0.086350) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.689804 (0.083205) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.692639 (0.082580) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.691789 (0.081296) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.696100 (0.083421) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.691273 (0.076712) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.695166 (0.080580) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.646151 (0.094434) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 30}\n",
      "0.663380 (0.094832) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 40}\n",
      "0.673366 (0.092284) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 50}\n",
      "0.675197 (0.088783) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 60}\n",
      "0.685824 (0.083821) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 70}\n",
      "0.688066 (0.074922) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 80}\n",
      "0.692166 (0.076815) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 90}\n",
      "0.695575 (0.076679) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 100}\n",
      "0.694068 (0.077312) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 110}\n",
      "0.691790 (0.084888) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 120}\n",
      "0.693456 (0.081938) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 130}\n",
      "0.697808 (0.085052) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 140}\n",
      "0.693046 (0.086467) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 150}\n",
      "0.694745 (0.081593) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 160}\n",
      "0.692588 (0.085535) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 170}\n",
      "0.695600 (0.081024) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 180}\n",
      "0.697662 (0.082908) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 190}\n",
      "0.648051 (0.089615) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 30}\n",
      "0.668072 (0.095683) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 40}\n",
      "0.668058 (0.085509) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 50}\n",
      "0.680576 (0.087077) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 60}\n",
      "0.686384 (0.084737) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 70}\n",
      "0.685877 (0.084580) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 80}\n",
      "0.688018 (0.086977) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 90}\n",
      "0.691996 (0.079862) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 100}\n",
      "0.697238 (0.082555) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 110}\n",
      "0.696140 (0.081035) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 120}\n",
      "0.691581 (0.086235) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 130}\n",
      "0.692338 (0.089205) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 140}\n",
      "0.693006 (0.083710) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 150}\n",
      "0.689173 (0.087954) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 160}\n",
      "0.696596 (0.089686) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 170}\n",
      "0.696626 (0.090122) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 180}\n",
      "0.696271 (0.083161) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 190}\n",
      "('precision',)\n",
      "0.975571 (0.038394) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 30}\n",
      "0.961990 (0.047394) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 40}\n",
      "0.947399 (0.057900) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 50}\n",
      "0.938876 (0.062872) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 60}\n",
      "0.933903 (0.061381) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 70}\n",
      "0.924388 (0.057081) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 80}\n",
      "0.908518 (0.069755) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 90}\n",
      "0.903764 (0.072871) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 100}\n",
      "0.898767 (0.073963) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 110}\n",
      "0.899774 (0.073512) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 120}\n",
      "0.891947 (0.078416) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 130}\n",
      "0.891200 (0.079559) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 140}\n",
      "0.884684 (0.078077) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 150}\n",
      "0.879295 (0.077317) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 160}\n",
      "0.879447 (0.076181) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 170}\n",
      "0.871937 (0.078201) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 180}\n",
      "0.871204 (0.078635) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 190}\n",
      "0.964299 (0.046848) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 30}\n",
      "0.959528 (0.053957) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 40}\n",
      "0.943399 (0.057855) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 50}\n",
      "0.936068 (0.061890) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 60}\n",
      "0.921146 (0.068128) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 70}\n",
      "0.918395 (0.066580) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 80}\n",
      "0.907404 (0.066844) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 90}\n",
      "0.898488 (0.071055) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 100}\n",
      "0.893370 (0.077812) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 110}\n",
      "0.883112 (0.079790) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 120}\n",
      "0.882718 (0.079110) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 130}\n",
      "0.872088 (0.077694) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 140}\n",
      "0.868762 (0.080299) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 150}\n",
      "0.863110 (0.075957) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 160}\n",
      "0.864759 (0.078216) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 170}\n",
      "0.859305 (0.079270) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 180}\n",
      "0.855010 (0.080537) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 190}\n",
      "0.962009 (0.047814) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 30}\n",
      "0.950009 (0.046094) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 40}\n",
      "0.938805 (0.052187) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 50}\n",
      "0.924512 (0.065598) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 60}\n",
      "0.916238 (0.068417) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 70}\n",
      "0.905383 (0.078185) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 80}\n",
      "0.901603 (0.074158) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 90}\n",
      "0.890965 (0.074586) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 100}\n",
      "0.876140 (0.075928) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 110}\n",
      "0.874079 (0.076168) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 120}\n",
      "0.870467 (0.079097) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 130}\n",
      "0.861249 (0.076992) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 140}\n",
      "0.859322 (0.079042) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 150}\n",
      "0.859852 (0.081713) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 160}\n",
      "0.856442 (0.075580) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 170}\n",
      "0.852354 (0.076449) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 180}\n",
      "0.850641 (0.081037) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 190}\n",
      "0.951135 (0.055323) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 30}\n",
      "0.931621 (0.053569) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 40}\n",
      "0.925836 (0.058242) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 50}\n",
      "0.902130 (0.069327) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 60}\n",
      "0.904177 (0.061036) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 70}\n",
      "0.892227 (0.069620) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 80}\n",
      "0.886197 (0.068637) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 90}\n",
      "0.875428 (0.075955) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 100}\n",
      "0.867647 (0.075040) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 110}\n",
      "0.862747 (0.080306) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 120}\n",
      "0.857375 (0.080644) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 130}\n",
      "0.852999 (0.084640) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 140}\n",
      "0.850631 (0.084217) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 150}\n",
      "0.849130 (0.087641) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 160}\n",
      "0.845106 (0.088497) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 170}\n",
      "0.842000 (0.087405) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 180}\n",
      "0.840694 (0.087646) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 190}\n",
      "0.941230 (0.058051) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.924131 (0.060538) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.910222 (0.063786) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.898568 (0.073153) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.887808 (0.072302) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.884999 (0.077603) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.875099 (0.076782) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.871286 (0.074015) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.865770 (0.079722) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.859888 (0.086745) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.858726 (0.083797) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.855589 (0.084221) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.859025 (0.085029) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.849439 (0.082938) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.852689 (0.078242) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.853308 (0.078741) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.845418 (0.084292) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.934408 (0.061650) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 30}\n",
      "0.922837 (0.059937) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 40}\n",
      "0.908581 (0.064674) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 50}\n",
      "0.902418 (0.062977) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 60}\n",
      "0.880985 (0.080531) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 70}\n",
      "0.867975 (0.081425) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 80}\n",
      "0.862310 (0.085089) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 90}\n",
      "0.862815 (0.079734) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 100}\n",
      "0.854673 (0.081803) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 110}\n",
      "0.859436 (0.078462) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 120}\n",
      "0.852948 (0.077087) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 130}\n",
      "0.849431 (0.087317) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 140}\n",
      "0.844888 (0.082249) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 150}\n",
      "0.839940 (0.081425) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 160}\n",
      "0.839892 (0.084006) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 170}\n",
      "0.839591 (0.079395) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 180}\n",
      "0.836804 (0.081456) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 190}\n",
      "0.923144 (0.058848) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.901238 (0.062871) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.898528 (0.073651) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.878760 (0.081215) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.869340 (0.085042) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.876277 (0.082362) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.869319 (0.081471) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.862229 (0.089828) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.862784 (0.081064) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.853809 (0.081385) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.842364 (0.077495) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.836518 (0.077468) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.838380 (0.081809) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.832367 (0.075664) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.829900 (0.078747) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.833216 (0.077276) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.837232 (0.083244) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.922992 (0.053692) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.901661 (0.062660) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.883004 (0.072112) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.873270 (0.071588) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.860820 (0.075193) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.854426 (0.078887) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.854332 (0.078508) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.849645 (0.069716) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.847428 (0.071774) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.836733 (0.074198) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.835043 (0.074527) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.838039 (0.069281) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.833738 (0.082165) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.835879 (0.075586) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.836649 (0.077174) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.826937 (0.080856) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.832671 (0.084382) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.918747 (0.052435) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 30}\n",
      "0.907008 (0.061733) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 40}\n",
      "0.885706 (0.080161) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 50}\n",
      "0.879056 (0.083126) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 60}\n",
      "0.877693 (0.070915) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 70}\n",
      "0.865353 (0.071712) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 80}\n",
      "0.858001 (0.078376) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 90}\n",
      "0.859698 (0.075562) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 100}\n",
      "0.851903 (0.077749) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 110}\n",
      "0.843600 (0.079501) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 120}\n",
      "0.843997 (0.072616) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 130}\n",
      "0.847517 (0.079379) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 140}\n",
      "0.837062 (0.078721) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 150}\n",
      "0.834060 (0.074616) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 160}\n",
      "0.823911 (0.078174) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 170}\n",
      "0.833113 (0.083959) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 180}\n",
      "0.830561 (0.088524) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 190}\n",
      "0.907633 (0.054746) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 30}\n",
      "0.895592 (0.068124) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 40}\n",
      "0.877157 (0.072175) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 50}\n",
      "0.873130 (0.070271) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 60}\n",
      "0.866624 (0.073244) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 70}\n",
      "0.853063 (0.072961) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 80}\n",
      "0.846679 (0.066361) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 90}\n",
      "0.848543 (0.071914) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 100}\n",
      "0.853708 (0.073699) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 110}\n",
      "0.849525 (0.077319) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 120}\n",
      "0.840047 (0.082967) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 130}\n",
      "0.837706 (0.085869) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 140}\n",
      "0.839018 (0.085585) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 150}\n",
      "0.826103 (0.088812) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 160}\n",
      "0.834024 (0.093753) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 170}\n",
      "0.833238 (0.091878) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 180}\n",
      "0.831460 (0.087999) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 190}\n",
      "('recall',)\n",
      "0.448192 (0.094311) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 30}\n",
      "0.453131 (0.094471) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 40}\n",
      "0.466623 (0.098020) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 50}\n",
      "0.485141 (0.100219) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 60}\n",
      "0.495018 (0.100284) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 70}\n",
      "0.509788 (0.103064) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 80}\n",
      "0.513492 (0.103531) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 90}\n",
      "0.523280 (0.104439) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 100}\n",
      "0.534347 (0.107213) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 110}\n",
      "0.540476 (0.102803) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 120}\n",
      "0.545326 (0.100559) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 130}\n",
      "0.557628 (0.101449) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 140}\n",
      "0.563757 (0.098495) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 150}\n",
      "0.564991 (0.101360) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 160}\n",
      "0.569885 (0.097463) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 170}\n",
      "0.571120 (0.098905) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 180}\n",
      "0.573589 (0.097552) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 190}\n",
      "0.450661 (0.094181) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 30}\n",
      "0.466623 (0.095180) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 40}\n",
      "0.487610 (0.099181) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 50}\n",
      "0.499956 (0.103104) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 60}\n",
      "0.512257 (0.103243) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 70}\n",
      "0.524559 (0.104818) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 80}\n",
      "0.531878 (0.102539) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 90}\n",
      "0.533113 (0.103039) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 100}\n",
      "0.545414 (0.103407) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 110}\n",
      "0.556437 (0.101110) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 120}\n",
      "0.560141 (0.097786) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 130}\n",
      "0.565035 (0.098274) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 140}\n",
      "0.566314 (0.104768) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 150}\n",
      "0.567504 (0.102117) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 160}\n",
      "0.569885 (0.100502) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 170}\n",
      "0.572266 (0.099427) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 180}\n",
      "0.573545 (0.104081) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 190}\n",
      "0.459259 (0.092957) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 30}\n",
      "0.481437 (0.100514) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 40}\n",
      "0.499956 (0.102660) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 50}\n",
      "0.509788 (0.103064) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 60}\n",
      "0.526984 (0.103544) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 70}\n",
      "0.536817 (0.099276) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 80}\n",
      "0.549118 (0.099885) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 90}\n",
      "0.552778 (0.096504) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 100}\n",
      "0.565035 (0.095919) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 110}\n",
      "0.568607 (0.093308) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 120}\n",
      "0.569841 (0.090890) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 130}\n",
      "0.573545 (0.094440) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 140}\n",
      "0.574824 (0.096542) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 150}\n",
      "0.577249 (0.093902) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 160}\n",
      "0.579718 (0.094374) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 170}\n",
      "0.584568 (0.094513) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 180}\n",
      "0.588183 (0.095010) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 190}\n",
      "0.465388 (0.095219) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 30}\n",
      "0.493783 (0.101124) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 40}\n",
      "0.508554 (0.103173) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 50}\n",
      "0.522090 (0.102283) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 60}\n",
      "0.541711 (0.106224) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 70}\n",
      "0.546649 (0.112182) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 80}\n",
      "0.556437 (0.099969) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 90}\n",
      "0.562478 (0.101859) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 100}\n",
      "0.571120 (0.102920) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 110}\n",
      "0.576058 (0.104705) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 120}\n",
      "0.577293 (0.105110) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 130}\n",
      "0.584568 (0.100702) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 140}\n",
      "0.588272 (0.097472) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 150}\n",
      "0.589462 (0.095716) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 160}\n",
      "0.590653 (0.101728) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 170}\n",
      "0.595591 (0.103042) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 180}\n",
      "0.593166 (0.102807) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 190}\n",
      "0.478968 (0.100026) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.496252 (0.100338) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.512213 (0.098505) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.530600 (0.101314) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.545370 (0.102009) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.557584 (0.096710) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.558818 (0.095487) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.568695 (0.100720) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.573589 (0.101169) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.577249 (0.101750) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.583333 (0.101568) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.582099 (0.099399) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.588228 (0.101340) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.594400 (0.101192) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.598016 (0.098096) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.604233 (0.101178) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.598060 (0.102816) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.486376 (0.096208) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 30}\n",
      "0.509788 (0.099451) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 40}\n",
      "0.530732 (0.107517) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 50}\n",
      "0.545414 (0.102706) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 60}\n",
      "0.556437 (0.102770) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 70}\n",
      "0.563801 (0.096404) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 80}\n",
      "0.572354 (0.097002) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 90}\n",
      "0.576058 (0.099785) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 100}\n",
      "0.579718 (0.093400) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 110}\n",
      "0.582187 (0.092267) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 120}\n",
      "0.583377 (0.092845) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 130}\n",
      "0.587081 (0.093383) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 140}\n",
      "0.593166 (0.094293) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 150}\n",
      "0.598060 (0.092516) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 160}\n",
      "0.601764 (0.094910) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 170}\n",
      "0.600573 (0.098703) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 180}\n",
      "0.599339 (0.098563) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 190}\n",
      "0.493783 (0.099758) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.514683 (0.101082) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.535582 (0.106162) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.552822 (0.111099) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.560141 (0.107373) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.568563 (0.090659) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.578439 (0.096106) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.580820 (0.098127) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.589418 (0.096054) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.590653 (0.096798) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.590608 (0.095223) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.596781 (0.096346) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.599250 (0.093800) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.596781 (0.098690) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.599250 (0.098554) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.599250 (0.095251) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.601720 (0.097890) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.502381 (0.092784) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.518386 (0.103013) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.541711 (0.106254) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.558951 (0.110407) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.568739 (0.111131) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.571164 (0.106893) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.582187 (0.110119) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.584700 (0.113315) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.585847 (0.110811) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.588316 (0.112163) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.591975 (0.105638) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.594444 (0.105624) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.600573 (0.103231) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.599383 (0.104551) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.604277 (0.105294) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.603042 (0.101220) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.605511 (0.102284) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.506085 (0.102718) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 30}\n",
      "0.531922 (0.107368) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 40}\n",
      "0.551499 (0.106075) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 50}\n",
      "0.556349 (0.104083) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 60}\n",
      "0.571076 (0.103156) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 70}\n",
      "0.578395 (0.094028) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 80}\n",
      "0.588272 (0.096902) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 90}\n",
      "0.591931 (0.095497) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 100}\n",
      "0.593122 (0.094908) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 110}\n",
      "0.594356 (0.102981) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 120}\n",
      "0.595591 (0.099566) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 130}\n",
      "0.600485 (0.099157) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 140}\n",
      "0.598016 (0.098861) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 150}\n",
      "0.602954 (0.099850) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 160}\n",
      "0.605423 (0.101383) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 170}\n",
      "0.605423 (0.098175) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 180}\n",
      "0.609127 (0.097854) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 190}\n",
      "0.512213 (0.101252) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 30}\n",
      "0.541799 (0.108232) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 40}\n",
      "0.547795 (0.100357) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 50}\n",
      "0.566270 (0.105987) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 60}\n",
      "0.577293 (0.105605) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 70}\n",
      "0.582231 (0.104903) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 80}\n",
      "0.588316 (0.108188) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 90}\n",
      "0.591975 (0.100014) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 100}\n",
      "0.596914 (0.101736) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 110}\n",
      "0.596914 (0.099132) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 120}\n",
      "0.595635 (0.104528) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 130}\n",
      "0.596869 (0.103826) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 140}\n",
      "0.596914 (0.098670) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 150}\n",
      "0.598104 (0.102308) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 160}\n",
      "0.605467 (0.105422) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 170}\n",
      "0.605467 (0.104988) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 180}\n",
      "0.606702 (0.101954) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 190}\n",
      "[12:07:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best: 0.919608 using {'XGboost__scale_pos_weight': 1}\n",
      "('accuracy',)\n",
      "0.919608 (0.017071) with: {'XGboost__scale_pos_weight': 1}\n",
      "0.918235 (0.019412) with: {'XGboost__scale_pos_weight': 2}\n",
      "0.919412 (0.015123) with: {'XGboost__scale_pos_weight': 3}\n",
      "0.917451 (0.019889) with: {'XGboost__scale_pos_weight': 4}\n",
      "0.917255 (0.018535) with: {'XGboost__scale_pos_weight': 5}\n",
      "0.916078 (0.017773) with: {'XGboost__scale_pos_weight': 6}\n",
      "('f1',)\n",
      "0.695412 (0.078089) with: {'XGboost__scale_pos_weight': 1}\n",
      "0.698036 (0.081617) with: {'XGboost__scale_pos_weight': 2}\n",
      "0.705418 (0.069681) with: {'XGboost__scale_pos_weight': 3}\n",
      "0.701240 (0.080737) with: {'XGboost__scale_pos_weight': 4}\n",
      "0.705202 (0.076278) with: {'XGboost__scale_pos_weight': 5}\n",
      "0.702289 (0.073446) with: {'XGboost__scale_pos_weight': 6}\n",
      "('precision',)\n",
      "0.871914 (0.074676) with: {'XGboost__scale_pos_weight': 1}\n",
      "0.845513 (0.088686) with: {'XGboost__scale_pos_weight': 2}\n",
      "0.841033 (0.067802) with: {'XGboost__scale_pos_weight': 3}\n",
      "0.826659 (0.088544) with: {'XGboost__scale_pos_weight': 4}\n",
      "0.811335 (0.073006) with: {'XGboost__scale_pos_weight': 5}\n",
      "0.806910 (0.078888) with: {'XGboost__scale_pos_weight': 6}\n",
      "('recall',)\n",
      "0.585758 (0.095762) with: {'XGboost__scale_pos_weight': 1}\n",
      "0.601543 (0.097169) with: {'XGboost__scale_pos_weight': 2}\n",
      "0.615168 (0.093029) with: {'XGboost__scale_pos_weight': 3}\n",
      "0.616490 (0.100459) with: {'XGboost__scale_pos_weight': 4}\n",
      "0.629718 (0.095725) with: {'XGboost__scale_pos_weight': 5}\n",
      "0.629806 (0.098098) with: {'XGboost__scale_pos_weight': 6}\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipelines(pipe1, pipe2, pipe3, pipe4, pipe5,pipe6,pipe7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST MODELS RE-EVALUATION WITH CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"SVM\", SVC(kernel='linear',C=0.17))])\n",
    "pipe2 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"Random Forest\", RandomForestClassifier(max_depth=8, random_state=1234,class_weight='balanced_subsample'))])\n",
    "pipe3 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"Decision Tree\", DecisionTreeClassifier(max_depth=2,random_state=1234,max_leaf_nodes=3))])\n",
    "pipe4 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"Logistic Regression\", LogisticRegression(penalty='l1',solver='liblinear',random_state=1234,max_iter=10000,C=0.35))])\n",
    "pipe5 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"AdaBoost\", AdaBoostClassifier(n_estimators=40, random_state=1234))])\n",
    "pipe6 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"Gradient Boosting\", GradientBoostingClassifier(n_estimators=180, learning_rate=0.09, random_state=1234))])\n",
    "pipe7 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"XGboost\", XGBClassifier(use_label_encoder=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [pipe1,pipe2,pipe3,pipe4,pipe5,pipe6,pipe7]\n",
    "results=[]\n",
    "names=['SVM','Random Forest','Decision Tree','Logistic Regression','AdaBoost','Gradient Boosting','XGboost']\n",
    "#models.append(pipe1)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "for model in models:\n",
    "    cv_results = cross_val_score(model, x, y, cv=cv,n_jobs=-1)\n",
    "    results.append(cv_results)\n",
    "#names.append(name)\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "fig.suptitle('Θηκογράμματα σύγκρισης ακρίβειας ταξινομητών')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:10:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best: 0.922353 using {'voting': 'hard', 'weights': (1, 2, 1)}\n",
      "('accuracy',)\n",
      "0.921765 (0.014097) with: {'voting': 'hard', 'weights': (1, 1, 1)}\n",
      "0.921373 (0.014996) with: {'voting': 'hard', 'weights': (2, 1, 1)}\n",
      "0.922353 (0.016457) with: {'voting': 'hard', 'weights': (1, 2, 1)}\n",
      "0.918235 (0.013358) with: {'voting': 'hard', 'weights': (1, 1, 2)}\n",
      "0.920196 (0.019253) with: {'voting': 'soft', 'weights': (1, 1, 1)}\n",
      "0.920784 (0.018523) with: {'voting': 'soft', 'weights': (2, 1, 1)}\n",
      "0.920784 (0.018954) with: {'voting': 'soft', 'weights': (1, 2, 1)}\n",
      "0.919804 (0.018999) with: {'voting': 'soft', 'weights': (1, 1, 2)}\n",
      "('f1',)\n",
      "0.705677 (0.062607) with: {'voting': 'hard', 'weights': (1, 1, 1)}\n",
      "0.694586 (0.071609) with: {'voting': 'hard', 'weights': (2, 1, 1)}\n",
      "0.699895 (0.077484) with: {'voting': 'hard', 'weights': (1, 2, 1)}\n",
      "0.682304 (0.066821) with: {'voting': 'hard', 'weights': (1, 1, 2)}\n",
      "0.697963 (0.087075) with: {'voting': 'soft', 'weights': (1, 1, 1)}\n",
      "0.699170 (0.082817) with: {'voting': 'soft', 'weights': (2, 1, 1)}\n",
      "0.701519 (0.085960) with: {'voting': 'soft', 'weights': (1, 2, 1)}\n",
      "0.696247 (0.085815) with: {'voting': 'soft', 'weights': (1, 1, 2)}\n",
      "('precision',)\n",
      "0.882548 (0.079251) with: {'voting': 'hard', 'weights': (1, 1, 1)}\n",
      "0.907545 (0.076216) with: {'voting': 'hard', 'weights': (2, 1, 1)}\n",
      "0.904875 (0.075588) with: {'voting': 'hard', 'weights': (1, 2, 1)}\n",
      "0.893326 (0.071002) with: {'voting': 'hard', 'weights': (1, 1, 2)}\n",
      "0.870497 (0.083675) with: {'voting': 'soft', 'weights': (1, 1, 1)}\n",
      "0.878552 (0.082671) with: {'voting': 'soft', 'weights': (2, 1, 1)}\n",
      "0.869283 (0.081820) with: {'voting': 'soft', 'weights': (1, 2, 1)}\n",
      "0.869906 (0.083264) with: {'voting': 'soft', 'weights': (1, 1, 2)}\n",
      "('recall',)\n",
      "0.595591 (0.082438) with: {'voting': 'hard', 'weights': (1, 1, 1)}\n",
      "0.569753 (0.087625) with: {'voting': 'hard', 'weights': (2, 1, 1)}\n",
      "0.578439 (0.094319) with: {'voting': 'hard', 'weights': (1, 2, 1)}\n",
      "0.559921 (0.087388) with: {'voting': 'hard', 'weights': (1, 1, 2)}\n",
      "0.589462 (0.102382) with: {'voting': 'soft', 'weights': (1, 1, 1)}\n",
      "0.586993 (0.095980) with: {'voting': 'soft', 'weights': (2, 1, 1)}\n",
      "0.595635 (0.103495) with: {'voting': 'soft', 'weights': (1, 2, 1)}\n",
      "0.586993 (0.100016) with: {'voting': 'soft', 'weights': (1, 1, 2)}\n"
     ]
    }
   ],
   "source": [
    "scoring = {'accuracy': 'accuracy','precision': 'precision','recall': 'recall','f1': 'f1'}\n",
    "params = {'voting':['hard', 'soft'],\n",
    "          'weights':[(1,1,1), (2,1,1), (1,2,1), (1,1,2)]}\n",
    "vc = VotingClassifier([('clf1', pipe7), ('clf2', pipe6), ('clf3', pipe5)])\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid = GridSearchCV(vc, params, n_jobs=-1, cv=cv,scoring=scoring,refit=\"accuracy\")\n",
    "grid_result =grid.fit(x, y)\n",
    "print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_))\n",
    "        \n",
    "        # report all configurations\n",
    "for scorer in zip(sorted(scoring)):\n",
    "    print(scorer)\n",
    "    means = grid_result.cv_results_['mean_test_%s' % scorer]\n",
    "    stds = grid_result.cv_results_['std_test_%s' % scorer]\n",
    "    params = grid_result.cv_results_['params']\n",
    "\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        \n",
    "        print('%f (%f) with: %r' % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOMEK LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('under', TomekLinks()),(\"SVM\", SVC(kernel='linear'))])\n",
    "pipe2 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('under', TomekLinks()),(\"Random Forest\", RandomForestClassifier(random_state=1234))])\n",
    "pipe3 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('under', TomekLinks()),(\"Decision Tree\", DecisionTreeClassifier(random_state=1234))])\n",
    "pipe4 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('under', TomekLinks()),(\"Logistic Regression\", LogisticRegression(solver='liblinear',random_state=1234,max_iter=10000))])\n",
    "pipe5 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('under', TomekLinks()),(\"AdaBoost\", AdaBoostClassifier( random_state=1234))])\n",
    "pipe6 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('under', TomekLinks()),(\"Gradient Boosting\", GradientBoostingClassifier( random_state=1234))])\n",
    "pipe7 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('under', TomekLinks()),(\"XGboost\", XGBClassifier(use_label_encoder=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.880196 using {'SVM__C': 0.09999999999999999}\n",
      "('accuracy',)\n",
      "0.868039 (0.016685) with: {'SVM__C': 0.01}\n",
      "0.872941 (0.019663) with: {'SVM__C': 0.02}\n",
      "0.875686 (0.022205) with: {'SVM__C': 0.03}\n",
      "0.877451 (0.022685) with: {'SVM__C': 0.04}\n",
      "0.878431 (0.023595) with: {'SVM__C': 0.05}\n",
      "0.879216 (0.025357) with: {'SVM__C': 0.060000000000000005}\n",
      "0.879216 (0.025629) with: {'SVM__C': 0.06999999999999999}\n",
      "0.879804 (0.025264) with: {'SVM__C': 0.08}\n",
      "0.879608 (0.025986) with: {'SVM__C': 0.09}\n",
      "0.880196 (0.025707) with: {'SVM__C': 0.09999999999999999}\n",
      "0.879412 (0.025449) with: {'SVM__C': 0.11}\n",
      "0.878824 (0.025578) with: {'SVM__C': 0.12}\n",
      "0.878039 (0.025092) with: {'SVM__C': 0.13}\n",
      "0.877843 (0.025034) with: {'SVM__C': 0.14}\n",
      "0.878039 (0.025092) with: {'SVM__C': 0.15000000000000002}\n",
      "0.876863 (0.024675) with: {'SVM__C': 0.16}\n",
      "0.876863 (0.024815) with: {'SVM__C': 0.17}\n",
      "0.875490 (0.025282) with: {'SVM__C': 0.18000000000000002}\n",
      "0.875098 (0.025716) with: {'SVM__C': 0.19}\n",
      "('f1',)\n",
      "0.333805 (0.122682) with: {'SVM__C': 0.01}\n",
      "0.406820 (0.126008) with: {'SVM__C': 0.02}\n",
      "0.446483 (0.121407) with: {'SVM__C': 0.03}\n",
      "0.468811 (0.115345) with: {'SVM__C': 0.04}\n",
      "0.477642 (0.120157) with: {'SVM__C': 0.05}\n",
      "0.488010 (0.126750) with: {'SVM__C': 0.060000000000000005}\n",
      "0.492119 (0.127273) with: {'SVM__C': 0.06999999999999999}\n",
      "0.501574 (0.120077) with: {'SVM__C': 0.08}\n",
      "0.504035 (0.120806) with: {'SVM__C': 0.09}\n",
      "0.508645 (0.119040) with: {'SVM__C': 0.09999999999999999}\n",
      "0.509250 (0.113688) with: {'SVM__C': 0.11}\n",
      "0.504285 (0.117562) with: {'SVM__C': 0.12}\n",
      "0.502952 (0.113768) with: {'SVM__C': 0.13}\n",
      "0.503415 (0.115110) with: {'SVM__C': 0.14}\n",
      "0.505927 (0.115481) with: {'SVM__C': 0.15000000000000002}\n",
      "0.502699 (0.114337) with: {'SVM__C': 0.16}\n",
      "0.504012 (0.113128) with: {'SVM__C': 0.17}\n",
      "0.501047 (0.112759) with: {'SVM__C': 0.18000000000000002}\n",
      "0.498355 (0.116486) with: {'SVM__C': 0.19}\n",
      "('precision',)\n",
      "0.813038 (0.211863) with: {'SVM__C': 0.01}\n",
      "0.777676 (0.149710) with: {'SVM__C': 0.02}\n",
      "0.763329 (0.147384) with: {'SVM__C': 0.03}\n",
      "0.757780 (0.151006) with: {'SVM__C': 0.04}\n",
      "0.757403 (0.152513) with: {'SVM__C': 0.05}\n",
      "0.747977 (0.156093) with: {'SVM__C': 0.060000000000000005}\n",
      "0.739817 (0.153427) with: {'SVM__C': 0.06999999999999999}\n",
      "0.736789 (0.152582) with: {'SVM__C': 0.08}\n",
      "0.731966 (0.156684) with: {'SVM__C': 0.09}\n",
      "0.732961 (0.154957) with: {'SVM__C': 0.09999999999999999}\n",
      "0.728111 (0.152738) with: {'SVM__C': 0.11}\n",
      "0.725519 (0.154172) with: {'SVM__C': 0.12}\n",
      "0.721786 (0.152442) with: {'SVM__C': 0.13}\n",
      "0.717946 (0.147665) with: {'SVM__C': 0.14}\n",
      "0.716489 (0.147075) with: {'SVM__C': 0.15000000000000002}\n",
      "0.708226 (0.146122) with: {'SVM__C': 0.16}\n",
      "0.706130 (0.146068) with: {'SVM__C': 0.17}\n",
      "0.695861 (0.146285) with: {'SVM__C': 0.18000000000000002}\n",
      "0.693358 (0.150206) with: {'SVM__C': 0.19}\n",
      "('recall',)\n",
      "0.215300 (0.093107) with: {'SVM__C': 0.01}\n",
      "0.284171 (0.110100) with: {'SVM__C': 0.02}\n",
      "0.324780 (0.113263) with: {'SVM__C': 0.03}\n",
      "0.347972 (0.108625) with: {'SVM__C': 0.04}\n",
      "0.358951 (0.115504) with: {'SVM__C': 0.05}\n",
      "0.372443 (0.122925) with: {'SVM__C': 0.060000000000000005}\n",
      "0.378616 (0.124332) with: {'SVM__C': 0.06999999999999999}\n",
      "0.388492 (0.115846) with: {'SVM__C': 0.08}\n",
      "0.392196 (0.115404) with: {'SVM__C': 0.09}\n",
      "0.397134 (0.113557) with: {'SVM__C': 0.09999999999999999}\n",
      "0.399603 (0.109243) with: {'SVM__C': 0.11}\n",
      "0.394665 (0.112497) with: {'SVM__C': 0.12}\n",
      "0.394665 (0.109615) with: {'SVM__C': 0.13}\n",
      "0.397090 (0.113358) with: {'SVM__C': 0.14}\n",
      "0.400794 (0.115431) with: {'SVM__C': 0.15000000000000002}\n",
      "0.399559 (0.114758) with: {'SVM__C': 0.16}\n",
      "0.400794 (0.112624) with: {'SVM__C': 0.17}\n",
      "0.399603 (0.110991) with: {'SVM__C': 0.18000000000000002}\n",
      "0.397134 (0.113642) with: {'SVM__C': 0.19}\n",
      "Best: 0.914510 using {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 7}\n",
      "('accuracy',)\n",
      "0.876078 (0.020821) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 2}\n",
      "0.890392 (0.020234) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 3}\n",
      "0.900196 (0.021344) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 4}\n",
      "0.908824 (0.017761) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 5}\n",
      "0.913529 (0.018796) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 6}\n",
      "0.914118 (0.017157) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 7}\n",
      "0.913137 (0.016959) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 8}\n",
      "0.914118 (0.017357) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 9}\n",
      "0.874314 (0.019714) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 2}\n",
      "0.890000 (0.022330) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 3}\n",
      "0.898431 (0.020542) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 4}\n",
      "0.906471 (0.020112) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 5}\n",
      "0.911569 (0.017809) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 6}\n",
      "0.914510 (0.015910) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 7}\n",
      "0.912157 (0.016633) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 8}\n",
      "0.912157 (0.014563) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 9}\n",
      "('f1',)\n",
      "0.603369 (0.067432) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 2}\n",
      "0.631278 (0.071151) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 3}\n",
      "0.650731 (0.080733) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 4}\n",
      "0.665241 (0.078770) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 5}\n",
      "0.670039 (0.087998) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 6}\n",
      "0.654491 (0.085058) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 7}\n",
      "0.640145 (0.084326) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 8}\n",
      "0.635596 (0.094816) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 9}\n",
      "0.599743 (0.064824) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 2}\n",
      "0.632090 (0.076529) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 3}\n",
      "0.647451 (0.077089) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 4}\n",
      "0.660669 (0.082287) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 5}\n",
      "0.661105 (0.081618) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 6}\n",
      "0.657625 (0.079565) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 7}\n",
      "0.639017 (0.082208) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 8}\n",
      "0.628114 (0.079027) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 9}\n",
      "('precision',)\n",
      "0.620045 (0.072356) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 2}\n",
      "0.685238 (0.085466) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 3}\n",
      "0.740777 (0.097716) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 4}\n",
      "0.798625 (0.076568) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 5}\n",
      "0.848014 (0.081127) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 6}\n",
      "0.903746 (0.075739) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 7}\n",
      "0.930779 (0.071598) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 8}\n",
      "0.961362 (0.052277) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 9}\n",
      "0.612383 (0.066401) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 2}\n",
      "0.682029 (0.092095) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 3}\n",
      "0.729317 (0.095778) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 4}\n",
      "0.782990 (0.091834) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 5}\n",
      "0.845482 (0.082835) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 6}\n",
      "0.901994 (0.071938) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 7}\n",
      "0.915211 (0.068317) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 8}\n",
      "0.954974 (0.060978) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 9}\n",
      "('recall',)\n",
      "0.594312 (0.086297) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 2}\n",
      "0.591799 (0.085541) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 3}\n",
      "0.588183 (0.094281) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 4}\n",
      "0.577116 (0.095337) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 5}\n",
      "0.561376 (0.101714) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 6}\n",
      "0.520767 (0.097524) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 7}\n",
      "0.493607 (0.092356) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 8}\n",
      "0.481437 (0.099140) with: {'Random Forest__class_weight': 'balanced', 'Random Forest__max_depth': 9}\n",
      "0.594312 (0.086297) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 2}\n",
      "0.595503 (0.088904) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 3}\n",
      "0.589418 (0.090117) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 4}\n",
      "0.578351 (0.096244) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 5}\n",
      "0.550176 (0.096141) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 6}\n",
      "0.524471 (0.091879) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 7}\n",
      "0.495988 (0.089106) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 8}\n",
      "0.473898 (0.086306) with: {'Random Forest__class_weight': 'balanced_subsample', 'Random Forest__max_depth': 9}\n",
      "Best: 0.910392 using {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 3}\n",
      "('accuracy',)\n",
      "0.880588 (0.016439) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.880588 (0.016439) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.909216 (0.015315) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.908039 (0.016392) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.906667 (0.016826) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.880588 (0.016439) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.909216 (0.015315) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905686 (0.015964) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.904314 (0.015558) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.880588 (0.016439) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.909216 (0.015315) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905686 (0.015964) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.904118 (0.015722) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.880588 (0.016439) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.909216 (0.015315) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905686 (0.015964) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.904118 (0.015722) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.880588 (0.016439) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.909216 (0.015315) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905686 (0.015964) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.904118 (0.015722) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.880588 (0.016439) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.909216 (0.015315) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905686 (0.015964) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.904118 (0.015722) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.880588 (0.016439) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.910392 (0.015540) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.909216 (0.015315) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.905686 (0.015964) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.904118 (0.015722) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 7}\n",
      "('f1',)\n",
      "0.590688 (0.065257) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.590688 (0.065257) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.615185 (0.083459) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.614019 (0.083019) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.611315 (0.083031) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.590688 (0.065257) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.615185 (0.083459) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.606921 (0.080344) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.605080 (0.079147) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.590688 (0.065257) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.615185 (0.083459) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.606921 (0.080344) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.605422 (0.078827) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.590688 (0.065257) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.615185 (0.083459) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.606921 (0.080344) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.605422 (0.078827) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.590688 (0.065257) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.615185 (0.083459) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.606921 (0.080344) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.605422 (0.078827) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.590688 (0.065257) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.615185 (0.083459) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.606921 (0.080344) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.605422 (0.078827) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.590688 (0.065257) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.612798 (0.087672) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.615185 (0.083459) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.606921 (0.080344) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.605422 (0.078827) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 7}\n",
      "('precision',)\n",
      "0.651290 (0.061229) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.651290 (0.061229) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.935889 (0.061150) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.919260 (0.081594) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.903919 (0.089242) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.651290 (0.061229) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.935889 (0.061150) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.898952 (0.084270) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.882667 (0.089499) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.651290 (0.061229) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.935889 (0.061150) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.898952 (0.084270) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.878554 (0.091012) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.651290 (0.061229) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.935889 (0.061150) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.898952 (0.084270) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.878554 (0.091012) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.651290 (0.061229) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.935889 (0.061150) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.898952 (0.084270) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.878554 (0.091012) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.651290 (0.061229) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.935889 (0.061150) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.898952 (0.084270) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.878554 (0.091012) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.651290 (0.061229) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.966737 (0.046703) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.935889 (0.061150) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.898952 (0.084270) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.878554 (0.091012) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 7}\n",
      "('recall',)\n",
      "0.545282 (0.082184) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 2, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.545282 (0.082184) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.464109 (0.088958) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.466578 (0.087858) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.467769 (0.087613) with: {'Decision Tree__max_depth': 3, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.545282 (0.082184) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.464109 (0.088958) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.464109 (0.086350) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.467813 (0.088834) with: {'Decision Tree__max_depth': 4, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.545282 (0.082184) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.464109 (0.088958) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.464109 (0.086350) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.469048 (0.087721) with: {'Decision Tree__max_depth': 5, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.545282 (0.082184) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.464109 (0.088958) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.464109 (0.086350) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.469048 (0.087721) with: {'Decision Tree__max_depth': 6, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.545282 (0.082184) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.464109 (0.088958) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.464109 (0.086350) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.469048 (0.087721) with: {'Decision Tree__max_depth': 7, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.545282 (0.082184) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.464109 (0.088958) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.464109 (0.086350) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.469048 (0.087721) with: {'Decision Tree__max_depth': 8, 'Decision Tree__max_leaf_nodes': 7}\n",
      "0.545282 (0.082184) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 2}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 3}\n",
      "0.454321 (0.090703) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 4}\n",
      "0.464109 (0.088958) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 5}\n",
      "0.464109 (0.086350) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 6}\n",
      "0.469048 (0.087721) with: {'Decision Tree__max_depth': 9, 'Decision Tree__max_leaf_nodes': 7}\n",
      "Best: 0.881569 using {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l1'}\n",
      "('accuracy',)\n",
      "0.873529 (0.025083) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l1'}\n",
      "0.877647 (0.024836) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l2'}\n",
      "0.876667 (0.026240) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l1'}\n",
      "0.880000 (0.024043) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l2'}\n",
      "0.880196 (0.025841) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.878039 (0.023818) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.879804 (0.026073) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l1'}\n",
      "0.877647 (0.024603) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l2'}\n",
      "0.880000 (0.026069) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.877451 (0.024400) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.879804 (0.025671) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l1'}\n",
      "0.877451 (0.024258) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l2'}\n",
      "0.881569 (0.023954) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l1'}\n",
      "0.876863 (0.024534) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l2'}\n",
      "0.880784 (0.022829) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l1'}\n",
      "0.877059 (0.024365) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l2'}\n",
      "('f1',)\n",
      "0.454336 (0.130712) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l1'}\n",
      "0.509296 (0.117316) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l2'}\n",
      "0.487356 (0.132408) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l1'}\n",
      "0.530674 (0.106726) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l2'}\n",
      "0.510022 (0.129208) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.526260 (0.104928) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.512432 (0.128493) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l1'}\n",
      "0.528361 (0.105756) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l2'}\n",
      "0.515651 (0.128806) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.529764 (0.105319) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.516483 (0.128456) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l1'}\n",
      "0.531307 (0.103513) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l2'}\n",
      "0.528266 (0.117779) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l1'}\n",
      "0.529608 (0.106019) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l2'}\n",
      "0.530922 (0.107345) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l1'}\n",
      "0.531379 (0.105757) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l2'}\n",
      "('precision',)\n",
      "0.714395 (0.156976) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l1'}\n",
      "0.698237 (0.127621) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l2'}\n",
      "0.707182 (0.144062) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l1'}\n",
      "0.703046 (0.122294) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l2'}\n",
      "0.718316 (0.131949) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.690889 (0.121698) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.713275 (0.134632) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l1'}\n",
      "0.686033 (0.123697) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l2'}\n",
      "0.710726 (0.135579) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.682309 (0.120322) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.705021 (0.128854) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l1'}\n",
      "0.682059 (0.119628) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l2'}\n",
      "0.713162 (0.115648) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l1'}\n",
      "0.678782 (0.120907) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l2'}\n",
      "0.707945 (0.112029) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l1'}\n",
      "0.678126 (0.119336) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l2'}\n",
      "('recall',)\n",
      "0.340697 (0.120785) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l1'}\n",
      "0.408289 (0.118807) with: {'Logistic Regression__C': 0.1, 'Logistic Regression__penalty': 'l2'}\n",
      "0.380026 (0.129032) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l1'}\n",
      "0.434127 (0.112694) with: {'Logistic Regression__C': 0.15000000000000002, 'Logistic Regression__penalty': 'l2'}\n",
      "0.404541 (0.131146) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.432892 (0.109902) with: {'Logistic Regression__C': 0.20000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.409436 (0.130993) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l1'}\n",
      "0.437743 (0.111324) with: {'Logistic Regression__C': 0.25000000000000006, 'Logistic Regression__penalty': 'l2'}\n",
      "0.414374 (0.132904) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l1'}\n",
      "0.441402 (0.112963) with: {'Logistic Regression__C': 0.30000000000000004, 'Logistic Regression__penalty': 'l2'}\n",
      "0.416799 (0.133522) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l1'}\n",
      "0.443871 (0.111782) with: {'Logistic Regression__C': 0.3500000000000001, 'Logistic Regression__penalty': 'l2'}\n",
      "0.429101 (0.127203) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l1'}\n",
      "0.443871 (0.115405) with: {'Logistic Regression__C': 0.40000000000000013, 'Logistic Regression__penalty': 'l2'}\n",
      "0.434083 (0.118339) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l1'}\n",
      "0.446340 (0.114994) with: {'Logistic Regression__C': 0.45000000000000007, 'Logistic Regression__penalty': 'l2'}\n",
      "Best: 0.913333 using {'AdaBoost__n_estimators': 40}\n",
      "('accuracy',)\n",
      "0.911961 (0.017081) with: {'AdaBoost__n_estimators': 30}\n",
      "0.913333 (0.017511) with: {'AdaBoost__n_estimators': 40}\n",
      "0.910588 (0.015958) with: {'AdaBoost__n_estimators': 50}\n",
      "0.909216 (0.015315) with: {'AdaBoost__n_estimators': 60}\n",
      "0.911176 (0.016241) with: {'AdaBoost__n_estimators': 70}\n",
      "0.910000 (0.016509) with: {'AdaBoost__n_estimators': 80}\n",
      "0.909804 (0.018310) with: {'AdaBoost__n_estimators': 90}\n",
      "0.908039 (0.018939) with: {'AdaBoost__n_estimators': 100}\n",
      "0.906471 (0.021605) with: {'AdaBoost__n_estimators': 110}\n",
      "0.906863 (0.021268) with: {'AdaBoost__n_estimators': 120}\n",
      "0.905098 (0.020305) with: {'AdaBoost__n_estimators': 130}\n",
      "0.903725 (0.020348) with: {'AdaBoost__n_estimators': 140}\n",
      "0.907059 (0.019296) with: {'AdaBoost__n_estimators': 150}\n",
      "0.904902 (0.020268) with: {'AdaBoost__n_estimators': 160}\n",
      "0.904902 (0.021322) with: {'AdaBoost__n_estimators': 170}\n",
      "0.905490 (0.020430) with: {'AdaBoost__n_estimators': 180}\n",
      "0.903725 (0.019479) with: {'AdaBoost__n_estimators': 190}\n",
      "('f1',)\n",
      "0.689935 (0.060786) with: {'AdaBoost__n_estimators': 30}\n",
      "0.691490 (0.068356) with: {'AdaBoost__n_estimators': 40}\n",
      "0.685618 (0.059940) with: {'AdaBoost__n_estimators': 50}\n",
      "0.681799 (0.056543) with: {'AdaBoost__n_estimators': 60}\n",
      "0.688786 (0.060321) with: {'AdaBoost__n_estimators': 70}\n",
      "0.685251 (0.061063) with: {'AdaBoost__n_estimators': 80}\n",
      "0.682712 (0.073091) with: {'AdaBoost__n_estimators': 90}\n",
      "0.675904 (0.074764) with: {'AdaBoost__n_estimators': 100}\n",
      "0.673177 (0.080060) with: {'AdaBoost__n_estimators': 110}\n",
      "0.672673 (0.079556) with: {'AdaBoost__n_estimators': 120}\n",
      "0.666602 (0.078073) with: {'AdaBoost__n_estimators': 130}\n",
      "0.662021 (0.076796) with: {'AdaBoost__n_estimators': 140}\n",
      "0.672119 (0.074423) with: {'AdaBoost__n_estimators': 150}\n",
      "0.669404 (0.075765) with: {'AdaBoost__n_estimators': 160}\n",
      "0.667221 (0.081783) with: {'AdaBoost__n_estimators': 170}\n",
      "0.667915 (0.080387) with: {'AdaBoost__n_estimators': 180}\n",
      "0.664171 (0.076786) with: {'AdaBoost__n_estimators': 190}\n",
      "('precision',)\n",
      "0.792816 (0.085269) with: {'AdaBoost__n_estimators': 30}\n",
      "0.802412 (0.086462) with: {'AdaBoost__n_estimators': 40}\n",
      "0.782126 (0.074749) with: {'AdaBoost__n_estimators': 50}\n",
      "0.775036 (0.069701) with: {'AdaBoost__n_estimators': 60}\n",
      "0.783251 (0.078904) with: {'AdaBoost__n_estimators': 70}\n",
      "0.777805 (0.079787) with: {'AdaBoost__n_estimators': 80}\n",
      "0.776433 (0.079976) with: {'AdaBoost__n_estimators': 90}\n",
      "0.771692 (0.083964) with: {'AdaBoost__n_estimators': 100}\n",
      "0.761696 (0.093810) with: {'AdaBoost__n_estimators': 110}\n",
      "0.766534 (0.092497) with: {'AdaBoost__n_estimators': 120}\n",
      "0.757810 (0.084704) with: {'AdaBoost__n_estimators': 130}\n",
      "0.754870 (0.092736) with: {'AdaBoost__n_estimators': 140}\n",
      "0.769729 (0.086116) with: {'AdaBoost__n_estimators': 150}\n",
      "0.753314 (0.090892) with: {'AdaBoost__n_estimators': 160}\n",
      "0.755810 (0.097084) with: {'AdaBoost__n_estimators': 170}\n",
      "0.759954 (0.093221) with: {'AdaBoost__n_estimators': 180}\n",
      "0.749512 (0.088449) with: {'AdaBoost__n_estimators': 190}\n",
      "('recall',)\n",
      "0.616402 (0.071294) with: {'AdaBoost__n_estimators': 30}\n",
      "0.613845 (0.080759) with: {'AdaBoost__n_estimators': 40}\n",
      "0.615168 (0.074509) with: {'AdaBoost__n_estimators': 50}\n",
      "0.612743 (0.067813) with: {'AdaBoost__n_estimators': 60}\n",
      "0.620062 (0.075609) with: {'AdaBoost__n_estimators': 70}\n",
      "0.617637 (0.073672) with: {'AdaBoost__n_estimators': 80}\n",
      "0.615168 (0.090032) with: {'AdaBoost__n_estimators': 90}\n",
      "0.607892 (0.091358) with: {'AdaBoost__n_estimators': 100}\n",
      "0.607716 (0.085674) with: {'AdaBoost__n_estimators': 110}\n",
      "0.604189 (0.086520) with: {'AdaBoost__n_estimators': 120}\n",
      "0.600485 (0.089128) with: {'AdaBoost__n_estimators': 130}\n",
      "0.595459 (0.085150) with: {'AdaBoost__n_estimators': 140}\n",
      "0.602822 (0.087129) with: {'AdaBoost__n_estimators': 150}\n",
      "0.607760 (0.084177) with: {'AdaBoost__n_estimators': 160}\n",
      "0.602822 (0.090143) with: {'AdaBoost__n_estimators': 170}\n",
      "0.601675 (0.090711) with: {'AdaBoost__n_estimators': 180}\n",
      "0.602822 (0.090143) with: {'AdaBoost__n_estimators': 190}\n",
      "Best: 0.918039 using {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 110}\n",
      "('accuracy',)\n",
      "0.910196 (0.016702) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 30}\n",
      "0.910784 (0.017455) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 40}\n",
      "0.911961 (0.016946) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 50}\n",
      "0.913333 (0.016633) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 60}\n",
      "0.913922 (0.016878) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 70}\n",
      "0.914314 (0.017429) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 80}\n",
      "0.914510 (0.017432) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 90}\n",
      "0.916078 (0.017511) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 100}\n",
      "0.915882 (0.018857) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 110}\n",
      "0.916863 (0.019670) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 120}\n",
      "0.917255 (0.018659) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 130}\n",
      "0.917451 (0.018878) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 140}\n",
      "0.917647 (0.018289) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 150}\n",
      "0.916863 (0.018461) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 160}\n",
      "0.917647 (0.018415) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 170}\n",
      "0.916471 (0.018314) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 180}\n",
      "0.916667 (0.017847) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 190}\n",
      "0.910784 (0.016850) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 30}\n",
      "0.911373 (0.016283) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 40}\n",
      "0.912745 (0.016574) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 50}\n",
      "0.913922 (0.017014) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 60}\n",
      "0.914314 (0.016194) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 70}\n",
      "0.913725 (0.016936) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 80}\n",
      "0.914510 (0.017889) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 90}\n",
      "0.915294 (0.018390) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 100}\n",
      "0.915882 (0.018045) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 110}\n",
      "0.917059 (0.017993) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 120}\n",
      "0.917647 (0.020434) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 130}\n",
      "0.916275 (0.019312) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 140}\n",
      "0.916667 (0.018294) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 150}\n",
      "0.916471 (0.018061) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 160}\n",
      "0.916667 (0.017054) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 170}\n",
      "0.916667 (0.017189) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 180}\n",
      "0.917647 (0.017647) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 190}\n",
      "0.911569 (0.016946) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 30}\n",
      "0.911961 (0.016392) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 40}\n",
      "0.913333 (0.016840) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 50}\n",
      "0.915294 (0.016679) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 60}\n",
      "0.915294 (0.017945) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 70}\n",
      "0.914902 (0.018585) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 80}\n",
      "0.915686 (0.018373) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 90}\n",
      "0.914706 (0.018586) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 100}\n",
      "0.916471 (0.018626) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 110}\n",
      "0.916667 (0.018040) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 120}\n",
      "0.916275 (0.018142) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 130}\n",
      "0.915686 (0.019351) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 140}\n",
      "0.915882 (0.019459) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 150}\n",
      "0.915490 (0.019060) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 160}\n",
      "0.914706 (0.019847) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 170}\n",
      "0.915098 (0.019072) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 180}\n",
      "0.915686 (0.019111) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 190}\n",
      "0.911961 (0.016671) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 30}\n",
      "0.913333 (0.016771) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 40}\n",
      "0.913922 (0.017416) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 50}\n",
      "0.915490 (0.018193) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 60}\n",
      "0.914902 (0.018709) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 70}\n",
      "0.915686 (0.019232) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 80}\n",
      "0.915882 (0.020498) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 90}\n",
      "0.916471 (0.019296) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 100}\n",
      "0.915490 (0.021235) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 110}\n",
      "0.914510 (0.020419) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 120}\n",
      "0.914902 (0.020134) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 130}\n",
      "0.914706 (0.019436) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 140}\n",
      "0.915294 (0.018762) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 150}\n",
      "0.915098 (0.019491) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 160}\n",
      "0.915686 (0.019470) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 170}\n",
      "0.916078 (0.019387) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 180}\n",
      "0.916471 (0.019710) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 190}\n",
      "0.912549 (0.016963) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.914902 (0.017498) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.915294 (0.017357) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.915686 (0.017472) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.915490 (0.018002) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.915294 (0.017423) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.915882 (0.018611) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.916275 (0.019193) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.916078 (0.019799) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.916275 (0.018829) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.915882 (0.020722) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.915882 (0.020889) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.915294 (0.020636) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.914706 (0.021141) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.914510 (0.019729) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.915098 (0.020472) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.915098 (0.019901) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.911765 (0.016707) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 30}\n",
      "0.913725 (0.018056) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 40}\n",
      "0.915686 (0.017538) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 50}\n",
      "0.917059 (0.018869) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 60}\n",
      "0.917843 (0.018319) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 70}\n",
      "0.917451 (0.018755) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 80}\n",
      "0.917647 (0.017647) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 90}\n",
      "0.917647 (0.018909) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 100}\n",
      "0.918039 (0.017313) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 110}\n",
      "0.917647 (0.019450) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 120}\n",
      "0.916667 (0.019632) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 130}\n",
      "0.916667 (0.019514) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 140}\n",
      "0.916275 (0.020016) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 150}\n",
      "0.916471 (0.021122) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 160}\n",
      "0.915686 (0.020111) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 170}\n",
      "0.915686 (0.021390) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 180}\n",
      "0.915686 (0.021118) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 190}\n",
      "0.914118 (0.017291) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.915098 (0.018332) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.916078 (0.017902) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.917059 (0.018121) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.916863 (0.017432) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.916863 (0.017954) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.916078 (0.019446) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.915686 (0.018929) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.914902 (0.019375) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.914510 (0.019553) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.914118 (0.021936) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.913725 (0.022850) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.914314 (0.022766) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.914118 (0.024282) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.914118 (0.022964) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.914706 (0.022257) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.914510 (0.022308) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.913333 (0.016424) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.914118 (0.018824) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.914902 (0.019729) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.916667 (0.020211) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.917059 (0.017865) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.916078 (0.017838) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.914902 (0.019196) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.915490 (0.019538) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.914902 (0.020248) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.914510 (0.020810) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.913333 (0.020987) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.913529 (0.019928) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.913725 (0.019938) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.915098 (0.020640) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.914902 (0.022256) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.915098 (0.021083) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.914314 (0.022254) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.914118 (0.017089) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 30}\n",
      "0.914706 (0.016896) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 40}\n",
      "0.915490 (0.017744) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 50}\n",
      "0.916078 (0.017643) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 60}\n",
      "0.915098 (0.016265) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 70}\n",
      "0.915686 (0.017928) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 80}\n",
      "0.916078 (0.017708) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 90}\n",
      "0.913725 (0.019292) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 100}\n",
      "0.915294 (0.020069) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 110}\n",
      "0.915882 (0.019812) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 120}\n",
      "0.915294 (0.020126) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 130}\n",
      "0.914706 (0.020532) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 140}\n",
      "0.914510 (0.020643) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 150}\n",
      "0.915490 (0.020685) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 160}\n",
      "0.914510 (0.021031) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 170}\n",
      "0.915098 (0.021888) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 180}\n",
      "0.915686 (0.021551) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 190}\n",
      "0.911961 (0.018319) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 30}\n",
      "0.915490 (0.020234) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 40}\n",
      "0.914706 (0.020532) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 50}\n",
      "0.914314 (0.021623) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 60}\n",
      "0.914314 (0.022665) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 70}\n",
      "0.914118 (0.022660) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 80}\n",
      "0.914706 (0.022412) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 90}\n",
      "0.912941 (0.023695) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 100}\n",
      "0.912353 (0.024694) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 110}\n",
      "0.912549 (0.024002) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 120}\n",
      "0.912353 (0.024973) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 130}\n",
      "0.911569 (0.023990) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 140}\n",
      "0.912353 (0.023983) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 150}\n",
      "0.912157 (0.023673) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 160}\n",
      "0.913137 (0.023068) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 170}\n",
      "0.911765 (0.025141) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 180}\n",
      "0.912549 (0.023516) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 190}\n",
      "('f1',)\n",
      "0.611810 (0.094832) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 30}\n",
      "0.617818 (0.097356) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 40}\n",
      "0.628365 (0.092778) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 50}\n",
      "0.640136 (0.091584) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 60}\n",
      "0.645514 (0.092502) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 70}\n",
      "0.651276 (0.090898) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 80}\n",
      "0.655421 (0.090984) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 90}\n",
      "0.664055 (0.092312) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 100}\n",
      "0.666515 (0.093985) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 110}\n",
      "0.672013 (0.098010) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 120}\n",
      "0.676368 (0.091842) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 130}\n",
      "0.677628 (0.092756) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 140}\n",
      "0.679161 (0.090135) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 150}\n",
      "0.679152 (0.088285) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 160}\n",
      "0.683791 (0.086547) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 170}\n",
      "0.682637 (0.085298) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 180}\n",
      "0.684022 (0.084070) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 190}\n",
      "0.616335 (0.093297) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 30}\n",
      "0.625344 (0.089610) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 40}\n",
      "0.636386 (0.092737) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 50}\n",
      "0.645381 (0.094427) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 60}\n",
      "0.652905 (0.088383) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 70}\n",
      "0.654170 (0.089821) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 80}\n",
      "0.658804 (0.092783) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 90}\n",
      "0.665881 (0.095916) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 100}\n",
      "0.671155 (0.091055) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 110}\n",
      "0.679085 (0.087361) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 120}\n",
      "0.684926 (0.091366) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 130}\n",
      "0.680745 (0.088258) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 140}\n",
      "0.684068 (0.085224) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 150}\n",
      "0.685808 (0.082705) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 160}\n",
      "0.688621 (0.077049) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 170}\n",
      "0.688968 (0.077994) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 180}\n",
      "0.692682 (0.078282) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 190}\n",
      "0.622848 (0.093371) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 30}\n",
      "0.633237 (0.089929) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 40}\n",
      "0.645083 (0.091986) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 50}\n",
      "0.659111 (0.086251) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 60}\n",
      "0.662924 (0.090733) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 70}\n",
      "0.663914 (0.092600) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 80}\n",
      "0.671073 (0.089636) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 90}\n",
      "0.670006 (0.087971) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 100}\n",
      "0.681117 (0.084293) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 110}\n",
      "0.682532 (0.082446) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 120}\n",
      "0.682391 (0.082487) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 130}\n",
      "0.681748 (0.085858) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 140}\n",
      "0.683695 (0.085628) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 150}\n",
      "0.683456 (0.084472) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 160}\n",
      "0.681382 (0.086854) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 170}\n",
      "0.683950 (0.083513) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 180}\n",
      "0.686870 (0.081329) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 190}\n",
      "0.628539 (0.092853) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 30}\n",
      "0.640280 (0.093523) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 40}\n",
      "0.649424 (0.092248) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 50}\n",
      "0.660357 (0.094972) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 60}\n",
      "0.664092 (0.090839) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 70}\n",
      "0.670459 (0.092337) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 80}\n",
      "0.675211 (0.094975) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 90}\n",
      "0.679794 (0.088149) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 100}\n",
      "0.679217 (0.092489) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 110}\n",
      "0.677646 (0.089638) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 120}\n",
      "0.681835 (0.086548) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 130}\n",
      "0.683158 (0.083865) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 140}\n",
      "0.686442 (0.079276) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 150}\n",
      "0.686819 (0.081994) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 160}\n",
      "0.690455 (0.083177) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 170}\n",
      "0.691896 (0.083284) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 180}\n",
      "0.696424 (0.079736) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 190}\n",
      "0.635914 (0.092146) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.652073 (0.092331) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.662764 (0.091524) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.667846 (0.089967) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.670039 (0.089597) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.673643 (0.083386) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.679219 (0.084660) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.683351 (0.085031) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.684741 (0.085481) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.686589 (0.083996) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.685918 (0.089614) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.687014 (0.088965) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.685373 (0.088942) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.684692 (0.090409) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.684065 (0.086852) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.687972 (0.087935) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.688482 (0.085792) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.635008 (0.090986) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 30}\n",
      "0.648956 (0.096747) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 40}\n",
      "0.664257 (0.088750) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 50}\n",
      "0.676566 (0.088774) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 60}\n",
      "0.684745 (0.082271) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 70}\n",
      "0.686103 (0.083118) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 80}\n",
      "0.689229 (0.077570) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 90}\n",
      "0.691735 (0.081046) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 100}\n",
      "0.694401 (0.078557) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 110}\n",
      "0.694669 (0.084717) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 120}\n",
      "0.693352 (0.085187) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 130}\n",
      "0.693581 (0.083868) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 140}\n",
      "0.693098 (0.085028) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 150}\n",
      "0.695106 (0.087142) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 160}\n",
      "0.693861 (0.081510) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 170}\n",
      "0.694633 (0.087049) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 180}\n",
      "0.694436 (0.086675) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 190}\n",
      "0.647682 (0.094204) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.656053 (0.096916) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.669083 (0.086298) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.679823 (0.084779) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.683947 (0.079664) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.685921 (0.080730) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.686637 (0.083968) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.686450 (0.082268) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.684312 (0.083884) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.682992 (0.085573) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.683779 (0.091266) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.683265 (0.093698) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.685825 (0.092818) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.687374 (0.095901) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.687289 (0.091762) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.690399 (0.091271) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.689974 (0.090582) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.644803 (0.091351) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.658402 (0.091776) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.667464 (0.090261) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.679779 (0.092053) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.684621 (0.083021) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.683347 (0.080701) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.681694 (0.083188) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.684774 (0.083352) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.683652 (0.085918) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.684734 (0.086473) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.682336 (0.087444) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.683839 (0.083608) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.684771 (0.083913) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.692043 (0.086164) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.691706 (0.090202) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.692089 (0.086779) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.690017 (0.089119) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.652517 (0.091640) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 30}\n",
      "0.663481 (0.085135) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 40}\n",
      "0.672370 (0.085842) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 50}\n",
      "0.681231 (0.080757) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 60}\n",
      "0.680960 (0.074429) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 70}\n",
      "0.685486 (0.077890) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 80}\n",
      "0.689757 (0.076352) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 90}\n",
      "0.684292 (0.079104) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 100}\n",
      "0.689838 (0.082856) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 110}\n",
      "0.693988 (0.080570) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 120}\n",
      "0.692770 (0.084987) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 130}\n",
      "0.690150 (0.085751) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 140}\n",
      "0.690958 (0.085712) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 150}\n",
      "0.693251 (0.084751) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 160}\n",
      "0.692394 (0.086193) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 170}\n",
      "0.694588 (0.087781) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 180}\n",
      "0.696521 (0.086654) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 190}\n",
      "0.647184 (0.087192) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 30}\n",
      "0.669599 (0.096308) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 40}\n",
      "0.674198 (0.092758) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 50}\n",
      "0.676872 (0.096402) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 60}\n",
      "0.680322 (0.099694) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 70}\n",
      "0.680876 (0.097174) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 80}\n",
      "0.686704 (0.093269) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 90}\n",
      "0.682268 (0.096748) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 100}\n",
      "0.683720 (0.096189) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 110}\n",
      "0.684001 (0.095094) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 120}\n",
      "0.685040 (0.094664) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 130}\n",
      "0.682025 (0.092711) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 140}\n",
      "0.685499 (0.091790) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 150}\n",
      "0.685100 (0.090738) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 160}\n",
      "0.687301 (0.091872) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 170}\n",
      "0.685431 (0.094547) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 180}\n",
      "0.686749 (0.090410) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 190}\n",
      "('precision',)\n",
      "0.961558 (0.047531) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 30}\n",
      "0.951900 (0.053785) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 40}\n",
      "0.942965 (0.060751) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 50}\n",
      "0.931693 (0.060760) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 60}\n",
      "0.923783 (0.058552) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 70}\n",
      "0.914944 (0.069715) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 80}\n",
      "0.905438 (0.070485) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 90}\n",
      "0.904428 (0.066847) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 100}\n",
      "0.891685 (0.074838) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 110}\n",
      "0.889191 (0.075718) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 120}\n",
      "0.884594 (0.071452) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 130}\n",
      "0.885287 (0.077578) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 140}\n",
      "0.882995 (0.071498) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 150}\n",
      "0.872723 (0.079338) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 160}\n",
      "0.871805 (0.080291) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 170}\n",
      "0.859323 (0.085647) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 180}\n",
      "0.858106 (0.082895) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 190}\n",
      "0.958669 (0.047328) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 30}\n",
      "0.943545 (0.062858) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 40}\n",
      "0.931547 (0.059353) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 50}\n",
      "0.922417 (0.057639) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 60}\n",
      "0.909042 (0.063916) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 70}\n",
      "0.894989 (0.072078) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 80}\n",
      "0.893213 (0.074481) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 90}\n",
      "0.883171 (0.073771) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 100}\n",
      "0.878667 (0.073730) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 110}\n",
      "0.877327 (0.078905) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 120}\n",
      "0.871183 (0.095518) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 130}\n",
      "0.861861 (0.091795) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 140}\n",
      "0.855173 (0.075421) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 150}\n",
      "0.850355 (0.081656) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 160}\n",
      "0.848098 (0.082212) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 170}\n",
      "0.845669 (0.078144) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 180}\n",
      "0.851144 (0.083655) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 190}\n",
      "0.953277 (0.053932) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 30}\n",
      "0.928891 (0.066701) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 40}\n",
      "0.916262 (0.070712) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 50}\n",
      "0.908755 (0.067668) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 60}\n",
      "0.895119 (0.075111) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 70}\n",
      "0.885061 (0.080009) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 80}\n",
      "0.877693 (0.078534) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 90}\n",
      "0.865975 (0.083221) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 100}\n",
      "0.864164 (0.086552) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 110}\n",
      "0.862164 (0.082090) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 120}\n",
      "0.857804 (0.084699) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 130}\n",
      "0.850951 (0.089983) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 140}\n",
      "0.848002 (0.090497) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 150}\n",
      "0.842078 (0.084658) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 160}\n",
      "0.836625 (0.087488) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 170}\n",
      "0.836465 (0.086004) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 180}\n",
      "0.838643 (0.087924) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 190}\n",
      "0.942645 (0.058347) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 30}\n",
      "0.929725 (0.059094) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 40}\n",
      "0.912315 (0.066618) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 50}\n",
      "0.903862 (0.069757) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 60}\n",
      "0.884147 (0.077080) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 70}\n",
      "0.877438 (0.076685) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 80}\n",
      "0.869617 (0.089658) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 90}\n",
      "0.867247 (0.086958) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 100}\n",
      "0.856268 (0.100060) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 110}\n",
      "0.844356 (0.091548) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 120}\n",
      "0.840686 (0.091653) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 130}\n",
      "0.835598 (0.092121) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 140}\n",
      "0.836676 (0.092154) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 150}\n",
      "0.832090 (0.093212) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 160}\n",
      "0.828510 (0.085794) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 170}\n",
      "0.830395 (0.089682) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 180}\n",
      "0.827712 (0.094459) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 190}\n",
      "0.930116 (0.065488) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.921385 (0.065834) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.896464 (0.072737) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.883900 (0.065243) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.876624 (0.076672) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.867236 (0.080467) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.861660 (0.085483) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.855337 (0.086570) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.849145 (0.089033) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.846028 (0.084990) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.842340 (0.093693) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.840210 (0.096376) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.834870 (0.091957) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.828552 (0.094762) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.826804 (0.087166) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.825391 (0.090740) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.824527 (0.086224) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.917701 (0.063180) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 30}\n",
      "0.906075 (0.061216) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 40}\n",
      "0.896150 (0.065977) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 50}\n",
      "0.883487 (0.078077) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 60}\n",
      "0.877633 (0.085591) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 70}\n",
      "0.866286 (0.084296) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 80}\n",
      "0.862336 (0.084923) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 90}\n",
      "0.856425 (0.090217) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 100}\n",
      "0.850287 (0.075332) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 110}\n",
      "0.844750 (0.089910) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 120}\n",
      "0.834007 (0.089251) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 130}\n",
      "0.834190 (0.089018) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 140}\n",
      "0.830315 (0.093182) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 150}\n",
      "0.829220 (0.097798) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 160}\n",
      "0.822648 (0.091155) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 170}\n",
      "0.820110 (0.095851) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 180}\n",
      "0.821106 (0.098347) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 190}\n",
      "0.917472 (0.059077) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.909577 (0.064977) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.890637 (0.074531) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.874586 (0.075039) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.861675 (0.074735) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.856302 (0.080157) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.841973 (0.082887) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.837036 (0.078167) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.831719 (0.080936) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.829287 (0.081871) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.822052 (0.091475) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.817978 (0.094011) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.820036 (0.094793) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.814237 (0.098423) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.815327 (0.094564) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.812884 (0.086080) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.813160 (0.088112) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.914257 (0.057218) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.893160 (0.084571) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.881431 (0.094855) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.870571 (0.089390) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.863121 (0.079784) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.853347 (0.081841) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.840875 (0.088291) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.841021 (0.087899) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.834646 (0.087950) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.826672 (0.088833) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.816726 (0.088508) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.816590 (0.087400) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.816426 (0.086542) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.815823 (0.086133) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.815111 (0.093227) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.817175 (0.089131) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.812765 (0.094973) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.904001 (0.059445) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 30}\n",
      "0.883495 (0.069800) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 40}\n",
      "0.872378 (0.075731) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 50}\n",
      "0.857201 (0.078372) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 60}\n",
      "0.847452 (0.082806) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 70}\n",
      "0.843180 (0.086541) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 80}\n",
      "0.837063 (0.084477) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 90}\n",
      "0.819728 (0.089627) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 100}\n",
      "0.827691 (0.095764) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 110}\n",
      "0.825420 (0.093374) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 120}\n",
      "0.818055 (0.088435) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 130}\n",
      "0.815812 (0.088780) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 140}\n",
      "0.812673 (0.091420) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 150}\n",
      "0.821149 (0.094044) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 160}\n",
      "0.809158 (0.093924) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 170}\n",
      "0.811006 (0.096190) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 180}\n",
      "0.814038 (0.095373) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 190}\n",
      "0.887452 (0.077707) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 30}\n",
      "0.876906 (0.084562) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 40}\n",
      "0.855385 (0.095904) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 50}\n",
      "0.842196 (0.102908) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 60}\n",
      "0.830597 (0.099003) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 70}\n",
      "0.827319 (0.098552) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 80}\n",
      "0.824101 (0.098056) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 90}\n",
      "0.812019 (0.105212) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 100}\n",
      "0.804838 (0.110557) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 110}\n",
      "0.806101 (0.107626) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 120}\n",
      "0.801808 (0.106723) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 130}\n",
      "0.799931 (0.106055) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 140}\n",
      "0.801638 (0.105550) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 150}\n",
      "0.801055 (0.106283) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 160}\n",
      "0.804737 (0.100244) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 170}\n",
      "0.795885 (0.109271) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 180}\n",
      "0.800592 (0.102827) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 190}\n",
      "('recall',)\n",
      "0.454365 (0.095073) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 30}\n",
      "0.463007 (0.096414) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 40}\n",
      "0.477734 (0.096497) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 50}\n",
      "0.495018 (0.097040) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 60}\n",
      "0.503660 (0.098965) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 70}\n",
      "0.513492 (0.099477) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 80}\n",
      "0.522134 (0.101134) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 90}\n",
      "0.533201 (0.102409) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 100}\n",
      "0.539286 (0.103089) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 110}\n",
      "0.547928 (0.109021) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 120}\n",
      "0.555291 (0.105830) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 130}\n",
      "0.557760 (0.107947) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 140}\n",
      "0.560185 (0.106963) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 150}\n",
      "0.563801 (0.103150) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 160}\n",
      "0.569885 (0.102047) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 170}\n",
      "0.574824 (0.103022) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 180}\n",
      "0.577249 (0.102261) with: {'Gradient Boosting__learning_rate': 0.05, 'Gradient Boosting__n_estimators': 190}\n",
      "0.459303 (0.092963) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 30}\n",
      "0.474074 (0.093350) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 40}\n",
      "0.491314 (0.100488) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 50}\n",
      "0.504894 (0.104749) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 60}\n",
      "0.518430 (0.102321) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 70}\n",
      "0.524603 (0.103253) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 80}\n",
      "0.530688 (0.105150) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 90}\n",
      "0.544224 (0.110372) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 100}\n",
      "0.551543 (0.105732) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 110}\n",
      "0.562610 (0.103548) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 120}\n",
      "0.572399 (0.104882) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 130}\n",
      "0.571164 (0.104418) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 140}\n",
      "0.577293 (0.102082) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 150}\n",
      "0.582143 (0.099116) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 160}\n",
      "0.587037 (0.093651) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 170}\n",
      "0.588228 (0.094125) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 180}\n",
      "0.590653 (0.092023) with: {'Gradient Boosting__learning_rate': 0.060000000000000005, 'Gradient Boosting__n_estimators': 190}\n",
      "0.467945 (0.092902) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 30}\n",
      "0.487610 (0.095899) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 40}\n",
      "0.506129 (0.099764) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 50}\n",
      "0.524559 (0.096339) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 60}\n",
      "0.534392 (0.101964) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 70}\n",
      "0.539286 (0.103531) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 80}\n",
      "0.551499 (0.103712) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 90}\n",
      "0.553924 (0.101042) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 100}\n",
      "0.568563 (0.095536) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 110}\n",
      "0.570988 (0.094562) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 120}\n",
      "0.573501 (0.096170) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 130}\n",
      "0.575970 (0.099426) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 140}\n",
      "0.579630 (0.099632) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 150}\n",
      "0.582099 (0.099922) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 160}\n",
      "0.582099 (0.102027) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 170}\n",
      "0.585758 (0.099250) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 180}\n",
      "0.588183 (0.095010) with: {'Gradient Boosting__learning_rate': 0.07, 'Gradient Boosting__n_estimators': 190}\n",
      "0.477778 (0.094096) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 30}\n",
      "0.496252 (0.100338) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 40}\n",
      "0.512257 (0.101004) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 50}\n",
      "0.528263 (0.103880) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 60}\n",
      "0.538007 (0.098040) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 70}\n",
      "0.549118 (0.101439) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 80}\n",
      "0.560229 (0.106159) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 90}\n",
      "0.566270 (0.099305) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 100}\n",
      "0.571208 (0.104499) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 110}\n",
      "0.573633 (0.103578) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 120}\n",
      "0.580952 (0.099814) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 130}\n",
      "0.585847 (0.098745) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 140}\n",
      "0.589506 (0.094320) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 150}\n",
      "0.591931 (0.095664) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 160}\n",
      "0.599339 (0.099945) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 170}\n",
      "0.600529 (0.099488) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 180}\n",
      "0.607892 (0.092709) with: {'Gradient Boosting__learning_rate': 0.08000000000000002, 'Gradient Boosting__n_estimators': 190}\n",
      "0.490079 (0.096660) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.512213 (0.100344) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.535626 (0.104897) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.544224 (0.102417) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.550309 (0.102827) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.558907 (0.098502) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.568739 (0.099468) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.576102 (0.098583) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.580952 (0.099814) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.585891 (0.102521) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.585891 (0.102521) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.588272 (0.101448) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.588316 (0.102515) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.590741 (0.104628) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.590785 (0.103033) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.596869 (0.102786) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.598060 (0.101021) with: {'Gradient Boosting__learning_rate': 0.09000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.492549 (0.096882) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 30}\n",
      "0.513492 (0.105100) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 40}\n",
      "0.534259 (0.097740) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 50}\n",
      "0.555159 (0.101470) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 60}\n",
      "0.568651 (0.095506) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 70}\n",
      "0.574824 (0.095657) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 80}\n",
      "0.580864 (0.091913) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 90}\n",
      "0.588272 (0.096902) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 100}\n",
      "0.594400 (0.097978) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 110}\n",
      "0.596737 (0.097716) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 120}\n",
      "0.600397 (0.099247) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 130}\n",
      "0.600441 (0.097254) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 140}\n",
      "0.601631 (0.097982) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 150}\n",
      "0.605335 (0.098734) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 160}\n",
      "0.606570 (0.095024) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 170}\n",
      "0.609039 (0.097952) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 180}\n",
      "0.609039 (0.099802) with: {'Gradient Boosting__learning_rate': 0.1, 'Gradient Boosting__n_estimators': 190}\n",
      "0.507319 (0.099928) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.520899 (0.103881) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.541667 (0.094054) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.562522 (0.097892) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.573457 (0.094125) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.578439 (0.094668) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.585802 (0.098168) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.588272 (0.097841) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.588228 (0.100009) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.588228 (0.103160) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.591931 (0.104789) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.593122 (0.106453) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.595591 (0.104680) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.600529 (0.106159) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.600573 (0.104705) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.606658 (0.106867) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.606746 (0.108145) with: {'Gradient Boosting__learning_rate': 0.11000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.506129 (0.101580) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 30}\n",
      "0.530688 (0.103139) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 40}\n",
      "0.545370 (0.099283) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 50}\n",
      "0.566314 (0.106069) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 60}\n",
      "0.576058 (0.102560) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 70}\n",
      "0.578483 (0.101023) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 80}\n",
      "0.580952 (0.100431) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 90}\n",
      "0.584568 (0.098771) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 100}\n",
      "0.585802 (0.100565) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 110}\n",
      "0.590741 (0.099310) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 120}\n",
      "0.593210 (0.103386) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 130}\n",
      "0.595679 (0.101105) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 140}\n",
      "0.596914 (0.100833) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 150}\n",
      "0.607937 (0.102675) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 160}\n",
      "0.607937 (0.104878) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 170}\n",
      "0.607937 (0.103561) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 180}\n",
      "0.606702 (0.103516) with: {'Gradient Boosting__learning_rate': 0.12000000000000001, 'Gradient Boosting__n_estimators': 190}\n",
      "0.518430 (0.101874) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 30}\n",
      "0.537963 (0.095609) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 40}\n",
      "0.555203 (0.100624) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 50}\n",
      "0.572310 (0.097419) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 60}\n",
      "0.577205 (0.093394) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 70}\n",
      "0.584524 (0.093842) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 80}\n",
      "0.593122 (0.092320) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 90}\n",
      "0.593122 (0.091824) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 100}\n",
      "0.598060 (0.095090) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 110}\n",
      "0.605379 (0.094336) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 120}\n",
      "0.607892 (0.099770) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 130}\n",
      "0.604145 (0.098057) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 140}\n",
      "0.607848 (0.099176) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 150}\n",
      "0.606570 (0.097095) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 160}\n",
      "0.611508 (0.099064) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 170}\n",
      "0.612743 (0.097657) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 180}\n",
      "0.613933 (0.096453) with: {'Gradient Boosting__learning_rate': 0.13, 'Gradient Boosting__n_estimators': 190}\n",
      "0.514638 (0.091003) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 30}\n",
      "0.549074 (0.106193) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 40}\n",
      "0.563801 (0.104032) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 50}\n",
      "0.573589 (0.107310) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 60}\n",
      "0.583466 (0.111505) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 70}\n",
      "0.584656 (0.107137) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 80}\n",
      "0.595723 (0.105631) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 90}\n",
      "0.594400 (0.104745) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 100}\n",
      "0.600485 (0.102491) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 110}\n",
      "0.600529 (0.102624) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 120}\n",
      "0.602998 (0.099228) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 130}\n",
      "0.601764 (0.104070) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 140}\n",
      "0.605379 (0.101796) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 150}\n",
      "0.605423 (0.101026) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 160}\n",
      "0.606658 (0.103767) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 170}\n",
      "0.609127 (0.104720) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 180}\n",
      "0.607848 (0.101905) with: {'Gradient Boosting__learning_rate': 0.14, 'Gradient Boosting__n_estimators': 190}\n",
      "[20:32:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best: 0.919216 using {'XGboost__scale_pos_weight': 2}\n",
      "('accuracy',)\n",
      "0.916471 (0.019826) with: {'XGboost__scale_pos_weight': 1}\n",
      "0.919216 (0.018721) with: {'XGboost__scale_pos_weight': 2}\n",
      "0.916078 (0.017112) with: {'XGboost__scale_pos_weight': 3}\n",
      "0.918824 (0.019296) with: {'XGboost__scale_pos_weight': 4}\n",
      "0.915882 (0.017460) with: {'XGboost__scale_pos_weight': 5}\n",
      "0.913529 (0.018734) with: {'XGboost__scale_pos_weight': 6}\n",
      "('f1',)\n",
      "0.689016 (0.080678) with: {'XGboost__scale_pos_weight': 1}\n",
      "0.703810 (0.076789) with: {'XGboost__scale_pos_weight': 2}\n",
      "0.697540 (0.070213) with: {'XGboost__scale_pos_weight': 3}\n",
      "0.709316 (0.077657) with: {'XGboost__scale_pos_weight': 4}\n",
      "0.703168 (0.073873) with: {'XGboost__scale_pos_weight': 5}\n",
      "0.697741 (0.071285) with: {'XGboost__scale_pos_weight': 6}\n",
      "('precision',)\n",
      "0.846252 (0.093477) with: {'XGboost__scale_pos_weight': 1}\n",
      "0.846507 (0.085048) with: {'XGboost__scale_pos_weight': 2}\n",
      "0.822177 (0.089733) with: {'XGboost__scale_pos_weight': 3}\n",
      "0.825234 (0.086723) with: {'XGboost__scale_pos_weight': 4}\n",
      "0.801972 (0.080796) with: {'XGboost__scale_pos_weight': 5}\n",
      "0.789418 (0.080022) with: {'XGboost__scale_pos_weight': 6}\n",
      "('recall',)\n",
      "0.588272 (0.097034) with: {'XGboost__scale_pos_weight': 1}\n",
      "0.610185 (0.093882) with: {'XGboost__scale_pos_weight': 2}\n",
      "0.613845 (0.089727) with: {'XGboost__scale_pos_weight': 3}\n",
      "0.629762 (0.097373) with: {'XGboost__scale_pos_weight': 4}\n",
      "0.633598 (0.095155) with: {'XGboost__scale_pos_weight': 5}\n",
      "0.632187 (0.090068) with: {'XGboost__scale_pos_weight': 6}\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipelines(pipe1, pipe2, pipe3, pipe4, pipe5,pipe6,pipe7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIDSEARCH PARAMETERS FOR PROBABILITY CALIBRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy': 'accuracy','precision': 'precision','recall': 'recall','f1': 'f1'}\n",
    "names=['SVM','Random Forest','Decision Tree','Logistic Regression','AdaBoost','Gradient Boosting','XGboost']\n",
    "models = [pipe1,pipe2,pipe3,pipe4,pipe5,pipe6,pipe7]\n",
    "for model,name in zip(models,names):\n",
    "    calibrated = CalibratedClassifierCV(model)\n",
    "    # define grid\n",
    "    param_grid = dict(cv=[2,3,4,5], method=['sigmoid','isotonic'])\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define grid search\n",
    "    grid = GridSearchCV(estimator=calibrated, param_grid=param_grid, n_jobs=-1, cv=cv,scoring=scoring,refit=\"accuracy\",return_train_score=True)\n",
    "    grid_result = grid.fit(x, y)\n",
    "    # report the best configuration\n",
    "    print(name)\n",
    "    print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_))\n",
    "    # report all configurations\n",
    "    for scorer in zip(sorted(scoring)):\n",
    "        print(scorer)\n",
    "        means = grid_result.cv_results_['mean_test_%s' % scorer]\n",
    "        stds = grid_result.cv_results_['std_test_%s' % scorer]\n",
    "        params = grid_result.cv_results_['params']\n",
    "        for mean, stdev, param in zip(means, stds, params):\n",
    "            print('%f (%f) with: %r' % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value imputation + min-max scaler on train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Αντικατασταση missing values με mean στηλης για στηλες με συνεχεις τιμες\n",
    "for col in numeric_features:\n",
    "        X_train[col].fillna(value=X_train[col].mean(),inplace=True)\n",
    "        X_test[col].fillna(value=X_test[col].mean(),inplace=True)\n",
    "#Αντικατασταση missing values με most common value (=mode) για τις υπολοιπες στηλες\n",
    "X_train=X_train.fillna(X_train.mode().iloc[0])\n",
    "X_test=X_test.fillna(X_test.mode().iloc[0])\n",
    "\n",
    "#min-max scaler\n",
    "X_train_norm=X_train.copy()\n",
    "X_test_norm=X_test.copy()\n",
    "for c in numeric_features:\n",
    "    norm=MinMaxScaler().fit(X_train_norm[[c]])    \n",
    "    X_train_norm[c]=norm.transform(X_train_norm[[c]])\n",
    "    X_test_norm[c]=norm.transform(X_test_norm[[c]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELIABILITY CURVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# predict calibrated probabilities\n",
    "def calibrated(X_train_norm, X_test_norm, y_train):\n",
    "\t# define model\n",
    "\tmodel = RandomForestClassifier(max_depth=7, random_state=1234,class_weight='balanced_subsample')\n",
    "\t# define and fit calibration model\n",
    "\tcalibrated = CalibratedClassifierCV(model, method='isotonic', cv=5)\n",
    "\tcalibrated.fit(X_train_norm, y_train)\n",
    "\t# predict probabilities\n",
    "\treturn calibrated.predict_proba(X_test_norm)[:, 1]\n",
    " \n",
    "\n",
    "yhat_calibrated = calibrated(X_train_norm, X_test_norm, y_train)\n",
    "\n",
    "fop_calibrated, mpv_calibrated = calibration_curve(y_test, yhat_calibrated, n_bins=10)\n",
    "# plot perfectly calibrated\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n",
    "plt.plot(mpv_calibrated, fop_calibrated, marker='.')\n",
    "plt.title(\"Καμπύλη αξιοπιστίας\")\n",
    "plt.xlabel(\"Mέση προβλεφθείσα πιθανότητα\")\n",
    "plt.ylabel(\"Σχετική συχνότητα κλάσης 1\")\n",
    "plt.legend([\"Perfect Calibration\",\"Random Forest\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# predict calibrated probabilities\n",
    "def calibrated(X_train_norm, X_test_norm, y_train):\n",
    "\t# define model\n",
    "\tmodel = XGBClassifier(use_label_encoder=False)\n",
    "\t# define and fit calibration model\n",
    "\tcalibrated = CalibratedClassifierCV(model, method='isotonic', cv=5)\n",
    "\tcalibrated.fit(X_train_norm, y_train)\n",
    "\t# predict probabilities\n",
    "\treturn calibrated.predict_proba(X_test_norm)[:, 1]\n",
    " \n",
    "\n",
    "yhat_calibrated = calibrated(X_train_norm, X_test_norm, y_train)\n",
    "\n",
    "fop_calibrated, mpv_calibrated = calibration_curve(y_test, yhat_calibrated, n_bins=10)\n",
    "# plot perfectly calibrated\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n",
    "plt.plot(mpv_calibrated, fop_calibrated, marker='.')\n",
    "plt.title(\"Καμπύλη αξιοπιστίας\")\n",
    "plt.xlabel(\"Mέση προβλεφθείσα πιθανότητα\")\n",
    "plt.ylabel(\"Σχετική συχνότητα κλάσης 1\")\n",
    "plt.legend([\"Perfect Calibration\",\"XGBoost\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holdout Set Evaluation + ROC + P-R CURVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    {\n",
    "        \"name\": \"SVM\",\n",
    "        \"classifier\": SVC(kernel='linear',C=0.16,probability=True)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Random Forest\",\n",
    "        \"classifier\": RandomForestClassifier(max_depth=7, random_state=1234,class_weight='balanced')\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Decision Tree\",\n",
    "        \"classifier\": DecisionTreeClassifier(max_depth=4,random_state=1234,max_leaf_nodes=4)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Logistic Regression\",\n",
    "        \"classifier\": LogisticRegression(penalty='l1',solver='liblinear',random_state=1234,max_iter=10000,C=0.35)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"AdaBoost\",\n",
    "        \"classifier\": AdaBoostClassifier(n_estimators=100, random_state=1234)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Gradient Boosting\",\n",
    "        \"classifier\": GradientBoostingClassifier(n_estimators=150, learning_rate=0.14, random_state=1234)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"XGboost\",\n",
    "        \"classifier\": XGBClassifier(use_label_encoder=False)\n",
    "    }\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))  # a new figure window\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_title('ROC curve')\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "ax.grid(True)\n",
    "\n",
    "\n",
    "fig2 = plt.figure(figsize=(10,6))\n",
    "ax2 = fig2.add_subplot(1, 1, 1)\n",
    "ax2.set_title('Precision-Recall curve')\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "\n",
    "\n",
    "for c in classifiers:\n",
    "    clf=c[\"classifier\"]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_true=y_test\n",
    "    print(\"classification report for {}\\n\".format(c[\"name\"]))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "\n",
    "    yhat = clf.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "    yhat = yhat[:, 1]\n",
    "\n",
    "#plot roc curve\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, yhat)\n",
    "    ax.plot(fpr, tpr,label=c[\"name\"]+\", AUC=%.3f\" % auc(fpr, tpr))\n",
    "    ax.legend()\n",
    "\n",
    "#plot precision-recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, yhat)\n",
    "    ax2.plot(recall, precision, marker='.', label=c[\"name\"]+\", AUC=%.3f\" % auc(recall, precision))\n",
    "    ax2.legend()\n",
    "\n",
    "#finally print Area Under the Curve of each precision-recall curve\n",
    "    print(\"AUC=%.3f\" % auc(recall, precision))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE-OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_c_range=np.arange(0.2,0.41,0.01)\n",
    "param_grid1 = {\"SVM__C\":svm_c_range}\n",
    "\n",
    "class_weights=['balanced','balanced_subsample']\n",
    "max_depth_range=np.arange(4,14)\n",
    "param_grid2 = {\"Random Forest__max_depth\":max_depth_range,\"Random Forest__class_weight\":class_weights}\n",
    "\n",
    "max_leaf_nodes_range=np.arange(3,12)\n",
    "param_grid3 = {\"Decision Tree__max_depth\":max_depth_range,\"Decision Tree__max_leaf_nodes\":max_leaf_nodes_range}\n",
    "\n",
    "log_reg_pen=[\"l1\",\"l2\"]\n",
    "log_reg_c=np.arange(0.1,0.5,0.05)\n",
    "param_grid4 = {\"Logistic Regression__penalty\":log_reg_pen,\"Logistic Regression__C\":log_reg_c}\n",
    "\n",
    "estimators=np.arange(50,200,10)\n",
    "param_grid5 ={\"AdaBoost__n_estimators\":estimators}\n",
    "\n",
    "grad_boost_lrate=np.arange(0.05,0.21,0.01)\n",
    "param_grid6 ={\"Gradient Boosting__n_estimators\":estimators,\"Gradient Boosting__learning_rate\":grad_boost_lrate}\n",
    "\n",
    "scale_pos_weight=np.arange(1,7)\n",
    "param_grid7 ={\"XGboost__scale_pos_weight\":scale_pos_weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"SMOTE_OVER\",SMOTE()),(\"SVM\", SVC(kernel='linear'))])\n",
    "pipe2 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"SMOTE_OVER\",SMOTE()),(\"Random Forest\", RandomForestClassifier(random_state=1234))])\n",
    "pipe3 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"SMOTE_OVER\",SMOTE()),(\"Decision Tree\", DecisionTreeClassifier(random_state=1234))])\n",
    "pipe4 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"SMOTE_OVER\",SMOTE()),(\"Logistic Regression\", LogisticRegression(solver='liblinear',random_state=1234,max_iter=10000))])\n",
    "pipe5 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"SMOTE_OVER\",SMOTE()),(\"AdaBoost\", AdaBoostClassifier( random_state=1234))])\n",
    "pipe6 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"SMOTE_OVER\",SMOTE()),(\"Gradient Boosting\", GradientBoostingClassifier( random_state=1234))])\n",
    "pipe7 = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"SMOTE_OVER\",SMOTE()),(\"XGboost\", XGBClassifier(use_label_encoder=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipelines(pipe1, pipe2, pipe3, pipe4, pipe5,pipe6,pipe7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomOverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_c_range=np.arange(0.1,0.2,0.01)\n",
    "param_grid1 = {\"SVM__C\":svm_c_range}\n",
    "\n",
    "class_weights=['balanced','balanced_subsample']\n",
    "max_depth_range=np.arange(4,10)\n",
    "param_grid2 = {\"Random Forest__max_depth\":max_depth_range,\"Random Forest__class_weight\":class_weights}\n",
    "\n",
    "max_leaf_nodes_range=np.arange(3,10)\n",
    "param_grid3 = {\"Decision Tree__max_depth\":max_depth_range,\"Decision Tree__max_leaf_nodes\":max_leaf_nodes_range}\n",
    "\n",
    "log_reg_pen=[\"l1\",\"l2\"]\n",
    "log_reg_c=np.arange(0.4,1,0.05)\n",
    "param_grid4 = {\"Logistic Regression__penalty\":log_reg_pen,\"Logistic Regression__C\":log_reg_c}\n",
    "\n",
    "estimators=np.arange(300,600,50)\n",
    "param_grid5 ={\"AdaBoost__n_estimators\":estimators}\n",
    "\n",
    "grad_boost_lrate=np.arange(0.05,0.21,0.01)\n",
    "param_grid6 ={\"Gradient Boosting__n_estimators\":estimators,\"Gradient Boosting__learning_rate\":grad_boost_lrate}\n",
    "\n",
    "scale_pos_weight=np.arange(1,7)\n",
    "param_grid7 ={\"XGboost__scale_pos_weight\":scale_pos_weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('over', RandomOverSampler(sampling_strategy='minority')),(\"SVM\", SVC(kernel='linear'))])\n",
    "pipe2 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('over', RandomOverSampler(sampling_strategy='minority')),(\"Random Forest\", RandomForestClassifier(random_state=1234))])\n",
    "pipe3 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('over', RandomOverSampler(sampling_strategy='minority')),(\"Decision Tree\", DecisionTreeClassifier(random_state=1234))])\n",
    "pipe4 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('over', RandomOverSampler(sampling_strategy='minority')),(\"Logistic Regression\", LogisticRegression(solver='liblinear',random_state=1234,max_iter=10000))])\n",
    "pipe5 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('over', RandomOverSampler(sampling_strategy='minority')),(\"AdaBoost\", AdaBoostClassifier( random_state=1234))])\n",
    "pipe6 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('over', RandomOverSampler(sampling_strategy='minority')),(\"Gradient Boosting\", GradientBoostingClassifier( random_state=1234))])\n",
    "pipe7 = Pipeline(steps=[(\"preprocessor\", preprocessor), ('over', RandomOverSampler(sampling_strategy='minority')),(\"XGboost\", XGBClassifier(use_label_encoder=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipelines(pipe1, pipe2, pipe3, pipe4, pipe5,pipe6,pipe7)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f87d4791dc38543bac2a10c0942e662659ddefc71c814c91be29721be80bc06f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
